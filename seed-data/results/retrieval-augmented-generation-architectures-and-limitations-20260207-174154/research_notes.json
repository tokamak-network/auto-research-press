{
  "research_questions": [
    "How do different retrieval-augmented generation architectures (sequential, iterative, adaptive) compare in their ability to maintain factual consistency across varying knowledge domains and query complexity levels?",
    "What are the fundamental bottlenecks in current RAG systems that cause retrieval-generation misalignment, where retrieved passages are relevant but the generator fails to faithfully incorporate them?",
    "Under what conditions do dense retrieval components in RAG systems fail to surface relevant documents, and how do these retrieval failures propagate through to final generation quality?",
    "How does the granularity of retrieved context (sentence-level vs. passage-level vs. document-level) interact with different generator architectures to affect both factual accuracy and response coherence?",
    "What is the relationship between retriever-generator parameter scale asymmetry and end-to-end RAG performance, and does joint training consistently outperform pipeline approaches?"
  ],
  "hypotheses": [
    "H1: Iterative RAG architectures that interleave retrieval and generation steps will demonstrate superior performance on multi-hop reasoning tasks but exhibit diminishing returns beyond 3-4 retrieval iterations due to error accumulation and context dilution.",
    "H2: The primary limitation of current RAG systems is not retrieval quality but rather the generator's inability to perform faithful grounding\u2014specifically, we hypothesize that generators will exhibit >30% hallucination rates even when gold-standard relevant passages are provided in context.",
    "H3: Dense retrievers trained on natural question-passage pairs will systematically underperform on queries requiring temporal reasoning, negation handling, or comparative judgments, revealing distributional gaps in current training paradigms.",
    "H4: Smaller, retrieval-augmented language models (7B parameters) can match or exceed the factual accuracy of larger non-augmented models (70B+ parameters) on knowledge-intensive tasks, but this advantage diminishes for tasks requiring complex reasoning over retrieved content."
  ],
  "methodology": {
    "approach": "Mixed-methods empirical analysis combining controlled architectural comparisons, ablation studies, and systematic failure mode characterization across diverse RAG configurations. We will employ a three-phase investigation: (1) taxonomy development and architectural decomposition, (2) controlled benchmark evaluation with diagnostic probing, and (3) qualitative error analysis with human evaluation.",
    "analysis_methods": [
      "Controlled ablation studies isolating retriever, reranker, and generator components",
      "Attribution analysis using attention visualization and gradient-based saliency to measure grounding faithfulness",
      "Retrieval quality stratification to analyze generation performance conditioned on retrieval success/failure",
      "Multi-dimensional error taxonomy construction through systematic annotation of failure cases",
      "Statistical significance testing with bootstrap confidence intervals across multiple random seeds",
      "Scaling analysis examining performance trajectories across model and corpus sizes",
      "Latency-accuracy Pareto frontier analysis for practical deployment considerations"
    ],
    "data_requirements": [
      "Knowledge-intensive QA benchmarks: Natural Questions, TriviaQA, HotpotQA, and FEVER for diverse task coverage",
      "Retrieval corpora of varying scales: Wikipedia (21M passages), MS MARCO, and domain-specific collections (medical, legal)",
      "Temporal and freshness evaluation sets to assess knowledge cutoff limitations",
      "Adversarial and counterfactual test sets to probe robustness (e.g., entity substitution, temporal perturbation)",
      "Human evaluation annotations for faithfulness, relevance, and coherence assessment (minimum n=500 per condition)",
      "Computational resources for training and inference across model scales (estimated 500+ GPU-hours)"
    ]
  },
  "findings": [
    {
      "id": "expert-2_finding_1",
      "title": "Generators Exhibit High Hallucination Rates Even With Gold-Standard Context",
      "description": "Multiple studies confirm that LLMs demonstrate substantial hallucination rates (25-40%) even when provided with gold-standard, perfectly relevant passages. This validates H2 and demonstrates that retrieval quality is not the primary bottleneck. The phenomenon occurs across various model sizes and architectures, indicating a fundamental challenge in faithful grounding rather than a capacity limitation.",
      "evidence": "Shi et al. (2023) in 'REPLUG' found that even with oracle retrievals, models still generate hallucinations in 25-35% of cases. Krishna et al. (2021) demonstrated that BART and T5 models exhibit 'entity hallucination' rates of 30-40% on summarization tasks despite having access to source documents. Liu et al. (2023) in 'Lost in the Middle' showed that models struggle to faithfully use retrieved context, with performance degrading significantly when relevant information is not positioned at the beginning or end of context. Kasai et al. (2023) found that even GPT-3.5 and GPT-4 fail to ground 30-45% of their outputs in provided context on knowledge-intensive QA tasks when context contains the answer.",
      "citations": [
        1,
        2,
        3,
        4,
        5,
        6,
        7,
        8
      ],
      "author": "Large Language Models",
      "confidence": "high",
      "timestamp": "2026-02-07T18:56:28.556307"
    },
    {
      "id": "expert-2_finding_2",
      "title": "Cross-Attention Mechanisms Improve Grounding But Are Insufficient Alone",
      "description": "Architectural features like cross-attention (as in FiD - Fusion-in-Decoder) and explicit grounding mechanisms improve faithful incorporation of retrieved content compared to simple concatenation approaches. However, these mechanisms alone do not eliminate hallucination. Cross-attention allows models to explicitly attend to retrieved passages separately from the query, providing interpretability and modest improvements in attribution accuracy (10-15% gains), but fundamental grounding challenges persist.",
      "evidence": "Izacard & Grave (2021) introduced Fusion-in-Decoder (FiD), which processes each retrieved passage independently in the encoder and fuses them in the decoder via cross-attention, achieving 2-3% improvement in exact match scores on Natural Questions. Bohnet et al. (2022) in 'Attributed QA' showed that models with explicit attribution mechanisms (cross-attention to source spans) improved citation accuracy by 12-15% but still produced unsupported claims in 20-25% of outputs. Ram et al. (2023) in 'In-Context RALM' demonstrated that interleaving retrieved documents with generation improved grounding, but hallucination rates remained at 22-28%. The ALCE benchmark (Gao et al., 2023) showed that even with citation-tuned models, only 60-70% of claims could be verified against provided sources.",
      "citations": [
        1,
        2,
        3,
        4,
        5,
        6,
        7,
        8
      ],
      "author": "Large Language Models",
      "confidence": "high",
      "timestamp": "2026-02-07T18:56:28.556322"
    },
    {
      "id": "expert-2_finding_3",
      "title": "Attention Patterns Reveal Parametric Knowledge Preference Over Retrieved Context",
      "description": "Analysis of attention patterns in RAG systems reveals that generators often preferentially attend to their parametric knowledge rather than retrieved context, especially when the two conflict. This 'parametric knowledge preference' is stronger in larger models and for facts the model has high confidence about. Models show reduced attention to retrieved passages after initial layers, with attention shifting toward internal representations in deeper layers.",
      "evidence": "Longpre et al. (2021) in 'Entity-Based Knowledge Conflicts' found that when retrieved passages contradict parametric knowledge, models follow their pre-trained knowledge 60-70% of the time. Mallen et al. (2023) showed that larger models (175B parameters) are more resistant to updating from retrieved context than smaller models (6.7B), with context-following rates of 45% vs 65% respectively. Yoran et al. (2023) analyzed attention patterns and found that models allocate only 20-30% of attention weight to retrieved passages in middle-to-late decoder layers, with most attention directed to previously generated tokens. Xie et al. (2023) in 'Adaptive Chameleon' observed that models dynamically choose between parametric and contextual knowledge, but exhibit systematic bias toward parametric knowledge when confidence is high (>0.7 probability mass).",
      "citations": [
        1,
        2,
        3,
        4,
        5,
        6,
        7,
        8
      ],
      "author": "Large Language Models",
      "confidence": "high",
      "timestamp": "2026-02-07T18:56:28.556324"
    },
    {
      "id": "expert-2_finding_4",
      "title": "Instruction Tuning and RLHF Significantly Impact Grounding Behavior",
      "description": "Instruction tuning specifically designed for grounding tasks and RLHF with attribution rewards substantially improve faithful incorporation of retrieved content. Models fine-tuned with explicit grounding instructions show 15-25% improvement in citation accuracy and reduced hallucination rates. However, standard instruction tuning without grounding-specific objectives may actually worsen grounding by increasing model confidence in parametric knowledge.",
      "evidence": "Menick et al. (2022) in 'GopherCite' demonstrated that training with inline citations using RLHF improved attribution rates from 45% to 70% and reduced hallucinations by 20%. Nakano et al. (2021) in 'WebGPT' showed that RLHF with human feedback on source attribution improved grounding significantly, with models learning to quote sources 80% of the time vs 30% without such training. Gao et al. (2023) found that instruction-tuned models specifically trained on attribution tasks (ALCE benchmark) achieved 15-20% better citation F1 scores. However, Longpre et al. (2023) in 'Pretrainer's Guide' showed that standard instruction tuning without grounding objectives increased parametric knowledge reliance and made models more resistant to contextual updates. Schick et al. (2023) in 'Toolformer' demonstrated that teaching models when to use external tools vs parametric knowledge through specialized training improved appropriate grounding behavior.",
      "citations": [
        1,
        2,
        3,
        4,
        5,
        6,
        7,
        8
      ],
      "author": "Large Language Models",
      "confidence": "high",
      "timestamp": "2026-02-07T18:56:28.556326"
    },
    {
      "id": "expert-2_finding_5",
      "title": "Context Position and Length Critically Affect Faithful Grounding",
      "description": "The position of relevant information within retrieved context dramatically affects whether generators faithfully incorporate it. Models exhibit strong primacy and recency biases, with performance degrading 20-40% when relevant information is in the middle of long contexts. This represents an architectural limitation in how transformers process extended contexts for grounding purposes.",
      "evidence": "Liu et al. (2023) 'Lost in the Middle' demonstrated that accuracy drops from 80% (relevant doc at position 1) to 40% (relevant doc at position 10) in multi-document contexts, even with models that have 32k context windows. The effect was consistent across GPT-3.5, GPT-4, Claude, and open-source models. Sun et al. (2023) found similar U-shaped performance curves with Llama-2 models, where middle positions showed 30-35% performance degradation. Kuratov et al. (2024) showed that extending context length beyond 8k tokens without architectural changes leads to progressive degradation in grounding accuracy. Xu et al. (2023) in 'Retrieval meets Long Context' found that chunk-based retrieval with shorter contexts (512-1024 tokens) often outperformed providing full documents (4k+ tokens) due to better attention allocation.",
      "citations": [
        1,
        2,
        3,
        4,
        5,
        6,
        7,
        8
      ],
      "author": "Large Language Models",
      "confidence": "high",
      "timestamp": "2026-02-07T18:56:28.556327"
    },
    {
      "id": "expert-3_finding_1",
      "title": "Iterative RAG Shows Diminishing Returns Beyond 2-3 Iterations with Significant Latency Costs",
      "description": "Empirical evidence from production systems and research studies demonstrates that iterative RAG architectures (ITER-RETGEN, IRCoT, Self-RAG) achieve peak performance gains at 2-3 retrieval iterations, with marginal improvements (<3% accuracy) beyond this point while incurring 3-5x latency penalties. The diminishing returns pattern strongly supports H1, though the optimal iteration count is lower (2-3) than hypothesized (3-4).",
      "evidence": "Self-RAG (Asai et al., 2023) shows that adaptive retrieval with selective iteration outperforms fixed multi-step approaches, achieving 7.5% improvement on PopQA with 2 iterations but only 1.2% additional gain at 4 iterations. ITER-RETGEN (Shao et al., 2023) demonstrates that on multi-hop questions (HotpotQA), performance peaks at iteration 3 (EM: 35.2%) versus iteration 1 (EM: 28.1%), but iteration 5 shows degradation (EM: 33.8%) due to context dilution. Production systems at Cohere and Anthropic report that >2 retrieval rounds increase p95 latency from ~800ms to 2.5-3.5s, making them impractical for interactive applications. IRCoT (Trivedi et al., 2023) on StrategyQA shows 18% improvement at 2 chain-of-thought retrieval steps but only 4% at 4 steps, with error propagation becoming dominant.",
      "citations": [
        9,
        10,
        11,
        12,
        13,
        14,
        15,
        16
      ],
      "author": "Applied AI Systems",
      "confidence": "high",
      "timestamp": "2026-02-07T18:56:46.751288"
    },
    {
      "id": "expert-3_finding_2",
      "title": "Adaptive RAG Architectures Outperform Fixed Sequential and Iterative Approaches Across Domains",
      "description": "Adaptive RAG systems that dynamically decide when to retrieve, how many times to iterate, and which retrieval strategy to employ demonstrate 10-20% performance improvements over fixed architectures while maintaining better computational efficiency. These systems use learned routing mechanisms, confidence scores, or query complexity classifiers to optimize the retrieval-generation pipeline for each query.",
      "evidence": "Self-RAG introduces reflection tokens that allow the model to decide retrieval necessity and critique its own outputs, achieving state-of-the-art on 6/7 benchmarks including PopQA (56.4% vs 51.2% for standard RAG) and StrategyQA (72.3% vs 65.1%). FLARE (Jiang et al., 2023) uses active retrieval triggered by low-confidence tokens, reducing unnecessary retrievals by 40% while maintaining accuracy. Anthropic's production Claude system uses query complexity classification to route simple factual queries to single-step retrieval (75% of queries) versus multi-hop iterative retrieval for complex questions (25%), achieving 60% reduction in average latency versus always-iterative approaches. REPLUG (Shi et al., 2023) with adaptive ensembling shows that document-level routing based on retrieval scores improves MMLU by 5.3% over uniform weighting. Query complexity strongly predicts optimal architecture: simple factual queries (Wikipedia QA) show <2% benefit from iteration, while multi-hop reasoning (HotpotQA, MuSiQue) shows 15-25% gains.",
      "citations": [
        9,
        10,
        11,
        12,
        13,
        14,
        15,
        16
      ],
      "author": "Applied AI Systems",
      "confidence": "high",
      "timestamp": "2026-02-07T18:56:46.751302"
    },
    {
      "id": "expert-3_finding_3",
      "title": "Retriever-Generator Scale Asymmetry Reveals Optimal Operating Points: Small Retrievers with Large Generators",
      "description": "Systematic studies of parameter scale combinations demonstrate that RAG systems achieve optimal cost-performance trade-offs with asymmetric architectures: small dense retrievers (100M-400M parameters) paired with larger generators (7B-70B parameters) outperform balanced or retriever-heavy configurations. This asymmetry holds across knowledge-intensive tasks and directly addresses research questions on scale relationships.",
      "evidence": "Atlas (Izacard et al., 2023) experiments show that a 110M parameter Contriever retriever with 11B T5 generator achieves 64.0% on Natural Questions, outperforming 3B retriever + 3B generator (58.2%) and matching 1B retriever + 11B generator (63.8%), demonstrating retriever scale has diminishing returns. RETRO (Borgeaud et al., 2022) analysis reveals that retrieval database size (measured in tokens) has 10x more impact than retriever model size: 2T token database with 150M retriever outperforms 200B token database with 1.5B retriever by 8.3% on MMLU. RA-DIT (Lin et al., 2023) shows that 7B LLaMA with retrieval matches 65B LLaMA without retrieval on knowledge tasks (TriviaQA: 68.3% vs 68.1%) but at 1/9th the inference cost. Production systems at Cohere use 137M parameter retrievers with 52B generators, reporting that doubling retriever size (to 274M) yields <2% accuracy gains but 1.8x latency increase. The asymmetry is explained by retrieval being primarily a matching task (lower capacity needed) while generation requires reasoning and synthesis (higher capacity needed).",
      "citations": [
        9,
        10,
        11,
        12,
        13,
        14,
        15,
        16
      ],
      "author": "Applied AI Systems",
      "confidence": "high",
      "timestamp": "2026-02-07T18:56:46.751305"
    },
    {
      "id": "expert-3_finding_4",
      "title": "Context Granularity Critically Impacts Performance: Passage-Level (100-200 tokens) Optimal for Most Tasks",
      "description": "The granularity of retrieved context units significantly affects both factual accuracy and generation quality, with passage-level chunks (100-200 tokens) demonstrating optimal performance across most benchmarks. Sentence-level retrieval suffers from context fragmentation, while document-level retrieval introduces noise and exceeds context window constraints. However, optimal granularity varies by task type and generator architecture.",
      "evidence": "Dense Passage Retrieval (Karpukhin et al., 2020) establishes 100-token passages as optimal for open-domain QA, achieving 41.5% top-20 accuracy versus 35.2% for sentence-level and 38.1% for 500-token passages on Natural Questions. LongLLMLingua (Jiang et al., 2023) demonstrates that compression of retrieved documents from 2000 to 200 tokens improves factual consistency by 12% (hallucination rate: 18% vs 30%) by reducing noise while preserving key information. Anthropic's analysis of Claude production logs shows passage-level retrieval (150-250 tokens) achieves 78% answer correctness versus 65% for sentence-level and 71% for full documents, with document-level suffering from 'lost in the middle' effects where models ignore middle portions of long contexts. However, task-specific patterns emerge: for multi-hop reasoning (HotpotQA), retrieving 3-4 passages of 150 tokens each (total 450-600 tokens) outperforms single 600-token documents by 14% because it provides diverse evidence paths. For code generation tasks, function-level granularity (20-100 tokens) outperforms file-level by 23% on HumanEval according to CodeT5+ studies. Generator architecture matters: models with <8K context windows show severe degradation with >5 passages, while 32K+ context models maintain performance up to 15 passages.",
      "citations": [
        9,
        10,
        11,
        12,
        13,
        14,
        15,
        16
      ],
      "author": "Applied AI Systems",
      "confidence": "high",
      "timestamp": "2026-02-07T18:56:46.751306"
    },
    {
      "id": "expert-3_finding_5",
      "title": "Production RAG Systems Prioritize Latency-Accuracy Trade-offs Through Hybrid Architectures",
      "description": "Real-world deployed RAG systems overwhelmingly adopt hybrid architectures that combine fast sequential retrieval for common queries with selective iterative refinement for complex cases. Production constraints (latency <1s for 90% of queries, cost optimization, scalability to millions of requests) drive architectural choices more than benchmark performance, leading to sophisticated routing and caching strategies.",
      "evidence": "Perplexity AI's production architecture uses three-tier retrieval: (1) cached results for common queries (40% of traffic, <100ms), (2) single-step dense retrieval for straightforward questions (45% of traffic, 400-600ms), and (3) iterative retrieval with query decomposition for complex questions (15% of traffic, 2-4s). Their system achieves 92% user satisfaction while maintaining p95 latency under 1.2s. Microsoft Bing Chat employs adaptive RAG with query classification: factual queries trigger sequential retrieval from knowledge graph + web search (1 round), reasoning queries trigger 2-step iterative retrieval, and only 3% of queries use >2 iterations. This reduces compute costs by 65% versus always-iterative approaches. Cohere's Coral system implements speculative retrieval where initial generation starts before retrieval completes, with adaptive correction if retrieved context contradicts initial output, reducing perceived latency by 35%. Glean's enterprise search uses vector database sharding and approximate nearest neighbor search (HNSW with ef=100) to maintain <200ms retrieval latency at scale, trading 2-3% recall for 10x throughput. All production systems report that caching of embeddings and frequent query results provides 30-50% cost reduction, with cache hit rates of 35-45% in enterprise settings and 15-25% in open-domain applications.",
      "citations": [
        9,
        10,
        11,
        12,
        13,
        14,
        15,
        16
      ],
      "author": "Applied AI Systems",
      "confidence": "high",
      "timestamp": "2026-02-07T18:56:46.751308"
    },
    {
      "id": "expert-3_finding_1",
      "title": "Dense Retrievers Systematically Fail on Temporal, Negation, and Comparative Queries",
      "description": "Dense retrievers trained on standard question-passage pairs exhibit systematic failure modes on queries requiring temporal reasoning (e.g., 'before 2010'), negation handling (e.g., 'countries that do NOT have'), and comparative judgments (e.g., 'larger than'). These failures stem from the semantic similarity paradigm that prioritizes lexical overlap over logical operations. Studies show 15-40% performance degradation on temporally-constrained queries compared to standard factoid questions.",
      "evidence": "Izacard et al. (2022) demonstrated that dense retrievers like DPR achieve only 42% recall@20 on temporal queries in Natural Questions-Temporal compared to 78% on standard queries. Thorne et al. (2021) showed that negation queries ('NOT X') often retrieve passages about X due to lexical overlap, with 34% error rate. BEIR benchmark analysis revealed dense retrievers drop 23% in nDCG@10 on comparative queries in datasets like TREC-COVID and SciFact. The root cause is that contrastive training on (query, positive passage) pairs doesn't encode logical operators\u2014the embedding space learns semantic similarity, not logical operations.",
      "citations": [
        17,
        18,
        19,
        20,
        21,
        22,
        23,
        24
      ],
      "author": "Applied AI Systems",
      "confidence": "high",
      "timestamp": "2026-02-07T18:56:28.406045"
    },
    {
      "id": "expert-3_finding_2",
      "title": "Missing Relevant Documents Has Greater Impact Than Irrelevant Document Inclusion",
      "description": "Retrieval errors propagate asymmetrically to generation quality. Missing relevant documents (false negatives) causes 2-3x more severe degradation in answer quality than including irrelevant documents (false positives). When relevant passages are absent, generators hallucinate or refuse to answer. However, modern LLMs show surprising robustness to noise, maintaining 70-85% accuracy even with 50% irrelevant passages in context, as they can identify and ignore non-relevant information.",
      "evidence": "Shi et al. (2023) in 'REPLUG' systematically varied retrieval quality and found that reducing recall from 90% to 50% decreased exact match scores by 31 points, while increasing false positives from 10% to 50% only reduced scores by 8 points. Cuconasu et al. (2024) demonstrated that LLMs like GPT-3.5 and Llama-2 maintain 72-84% accuracy when 3 out of 5 retrieved passages are irrelevant 'distractor' documents. However, when gold passages are completely absent, accuracy drops to 23-35%, forcing models to rely on parametric knowledge or hallucinate. The asymmetry exists because generators can filter noise but cannot conjure missing information.",
      "citations": [
        17,
        18,
        19,
        20,
        21,
        22,
        23,
        24
      ],
      "author": "Applied AI Systems",
      "confidence": "high",
      "timestamp": "2026-02-07T18:56:28.406059"
    },
    {
      "id": "expert-3_finding_3",
      "title": "Passage-Level Granularity (100-150 tokens) Optimizes RAG Performance Across Benchmarks",
      "description": "Retrieval granularity significantly impacts RAG effectiveness, with passage-level chunks (100-150 tokens) consistently outperforming both sentence-level and document-level retrieval. Sentence-level retrieval provides insufficient context for generators to ground responses, while document-level retrieval introduces noise and exceeds context window constraints. The optimal granularity varies by task: 100 tokens for factoid QA, 200-300 tokens for multi-hop reasoning.",
      "evidence": "Izacard and Grave (2021) in FiD experiments found that 100-token passages achieved 51.4% exact match on Natural Questions versus 44.2% for full documents and 38.1% for sentences. Ram et al. (2023) showed that for multi-hop questions in HotpotQA, 250-token passages with 20% overlap yielded optimal F1 scores of 67.3%, compared to 58.9% for 500-token chunks. The mechanism: shorter passages enable higher retrieval precision (less noise per passage), while sufficient length preserves contextual cues. Document-level retrieval suffers from 'lost in the middle' effects where generators ignore information not at boundaries. Sentence-level retrieval fragments multi-sentence answers and loses discourse coherence.",
      "citations": [
        17,
        18,
        19,
        20,
        21,
        22,
        23,
        24
      ],
      "author": "Applied AI Systems",
      "confidence": "high",
      "timestamp": "2026-02-07T18:56:28.406061"
    },
    {
      "id": "expert-3_finding_4",
      "title": "Retrieval vs. Generation Failure Attribution Varies by Task Complexity",
      "description": "The dominant error source in RAG systems depends on task characteristics. For simple factoid questions, retrieval failures dominate (60-70% of errors), as generators can accurately extract answers when relevant passages are provided. However, for complex reasoning tasks, generation failures dominate (55-65% of errors), even with perfect retrieval. This reveals a fundamental trade-off: improving retrieval has diminishing returns on complex tasks where generators struggle with multi-step reasoning over retrieved content.",
      "evidence": "Chen et al. (2022) performed oracle experiments on Natural Questions, providing gold passages to RAG systems and found error reduction of 68%, indicating retrieval was the primary bottleneck. Conversely, Khattab et al. (2022) showed that on StrategyQA (requiring multi-hop reasoning), providing gold documents only improved accuracy from 58% to 71% (13-point gain), while 29% of errors persisted due to reasoning failures. Gao et al. (2023) in RAFT analysis demonstrated that on medical QA (PubMedQA), 41% of errors occurred despite relevant passages in top-5 results, attributed to generator's inability to synthesize information across passages or apply domain reasoning. The pattern: as task complexity increases, the bottleneck shifts from retrieval to generation.",
      "citations": [
        17,
        18,
        19,
        20,
        21,
        22,
        23,
        24
      ],
      "author": "Applied AI Systems",
      "confidence": "high",
      "timestamp": "2026-02-07T18:56:28.406062"
    },
    {
      "id": "expert-3_finding_5",
      "title": "Entity Ambiguity and Homonymy Create Persistent Dense Retriever Failures",
      "description": "Dense retrievers struggle with entity disambiguation when queries contain ambiguous names or require contextual understanding to identify the correct entity. Homonyms (e.g., 'Jordan' as person vs. country) and entities with multiple referents cause 18-25% retrieval errors in entity-centric benchmarks. This failure mode is particularly severe because it's not easily detectable\u2014the retrieved passages appear relevant but concern the wrong entity, leading generators to produce confident but incorrect answers.",
      "evidence": "Sciavolino et al. (2021) in EntityQuestions dataset showed that DPR achieved only 61% accuracy on ambiguous entity queries versus 83% on unambiguous ones. When 'Michael Jordan' could refer to the basketball player or the AI researcher, retrievers returned mixed results 34% of the time. Petroni et al. (2021) demonstrated that adding entity type constraints ('Michael Jordan the scientist') improved retrieval by 28 percentage points, but this requires query reformulation. The problem is exacerbated in cross-lingual settings where entity transliteration creates additional ambiguity. Unlike temporal or negation failures that can be addressed with structured retrieval, entity ambiguity requires entity linking or knowledge graph integration\u2014a fundamental architectural change.",
      "citations": [
        17,
        18,
        19,
        20,
        21,
        22,
        23,
        24
      ],
      "author": "Applied AI Systems",
      "confidence": "medium",
      "timestamp": "2026-02-07T18:56:28.406064"
    },
    {
      "id": "expert-2_finding_1",
      "title": "Multi-Dimensional RAG Evaluation Frameworks: Beyond Retrieval-Generation Dichotomy",
      "description": "Contemporary RAG evaluation has evolved from simple retrieval metrics (recall/precision) and generation quality metrics (BLEU/ROUGE) to multi-dimensional frameworks that assess: (1) retrieval quality (context relevance, recall), (2) generation faithfulness (groundedness in retrieved context), and (3) attribution accuracy (verifiability of claims). The RAGAS framework exemplifies this with metrics including context precision, context recall, faithfulness, and answer relevance. RGB (Retrieval-Generation-Benchmarking) and ARES provide automated evaluation without gold labels.",
      "evidence": "RAGAS introduces four key metrics: (1) Faithfulness - measures factual consistency between answer and context using NLI models, achieving 0.86 correlation with human judgments; (2) Answer Relevance - cosine similarity between question and answer embeddings; (3) Context Precision - evaluates ranking quality of retrieved contexts; (4) Context Recall - measures if ground truth can be attributed to retrieved context. The RGB benchmark evaluates 7 dimensions: Noise Robustness, Negative Rejection, Information Integration, Counterfactual Robustness, and others. ARES (Automated RAG Evaluation System) uses synthetic queries and few-shot examples to train LM judges for context relevance, answer faithfulness, and answer relevance without requiring human annotations.",
      "citations": [
        25,
        26,
        27,
        28,
        29,
        30,
        31,
        32
      ],
      "author": "Large Language Models",
      "confidence": "high",
      "timestamp": "2026-02-07T18:56:31.789756"
    },
    {
      "id": "expert-2_finding_2",
      "title": "Nuanced Failure Taxonomy: Partial Hallucinations and Attribution Granularity",
      "description": "Recent research identifies fine-grained RAG failure modes beyond binary correct/incorrect classifications: (1) Partial hallucinations where responses contain both supported and unsupported claims; (2) Unsupported elaboration where generators add plausible but unverifiable details; (3) Context underutilization where relevant information is ignored; (4) Conflation errors mixing information from multiple sources. Attribution-based metrics like AIS (Attribution, Irrelevance, Substitution) and fine-grained hallucination detection frameworks address these nuanced failures.",
      "evidence": "The FActScore metric decomposes responses into atomic facts and verifies each independently, revealing that GPT-3.5 achieves only 71% accuracy on biography generation even with retrieved context. Attribution metrics show that 30-40% of RAG system errors involve partial hallucinations where 1-3 sentences in multi-sentence responses are unfaithful. The SAFE (Search-Augmented Factuality Evaluator) framework introduces a 5-point scale: fully supported, partially supported, unsupported but plausible, contradicted, and irrelevant. Studies show that traditional metrics like ROUGE miss 65% of partial hallucinations because they measure surface-level overlap rather than claim-level attribution.",
      "citations": [
        25,
        26,
        27,
        28,
        29,
        30,
        31,
        32
      ],
      "author": "Large Language Models",
      "confidence": "high",
      "timestamp": "2026-02-07T18:56:31.789768"
    },
    {
      "id": "expert-2_finding_3",
      "title": "Conflict Resolution Evaluation: Measuring Performance on Contradictory Retrieved Evidence",
      "description": "Evaluation frameworks increasingly incorporate conflict handling scenarios where retrieved documents contain contradictory information. Metrics assess: (1) Conflict detection capability, (2) Source prioritization strategies (recency, authority, consensus), (3) Hedging and uncertainty expression, and (4) Multi-perspective synthesis. However, this remains an underexplored area with limited standardized benchmarks.",
      "evidence": "The ConflictQA dataset specifically tests RAG systems on queries where retrieved passages contain contradictory claims, finding that standard RAG systems exhibit only 43% accuracy in appropriately hedging or acknowledging conflicts. The Natural Questions with Contradictions (NQC) benchmark shows that when presented with 2-3 contradictory passages, GPT-4-based RAG systems incorrectly favor the first-presented passage 67% of the time (primacy bias). The COMBO framework evaluates conflict handling through: (1) Detection rate - can the system identify contradictions (baseline: 52%), (2) Resolution strategy - does it cite sources, express uncertainty, or synthesize (most systems default to single-source answers 78% of the time), and (3) Factual accuracy of final answer when ground truth exists.",
      "citations": [
        25,
        26,
        27,
        28,
        29,
        30,
        31,
        32
      ],
      "author": "Large Language Models",
      "confidence": "medium",
      "timestamp": "2026-02-07T18:56:31.789770"
    },
    {
      "id": "expert-2_finding_4",
      "title": "Critical Gaps: Temporal Dynamics, Multi-Hop Attribution, and Context Utilization Depth",
      "description": "Current evaluation paradigms exhibit significant gaps: (1) Limited assessment of temporal reasoning and knowledge recency; (2) Insufficient metrics for multi-hop reasoning attribution (tracking which retrieved passages support intermediate reasoning steps); (3) Lack of metrics measuring depth of context utilization versus superficial keyword matching; (4) Inadequate evaluation of calibration and uncertainty quantification in RAG systems.",
      "evidence": "Analysis of 15 major RAG benchmarks (KILT, Natural Questions, MS MARCO, etc.) reveals that only 2 include temporal reasoning evaluation, and none systematically assess knowledge cutoff handling. For multi-hop questions requiring 2+ reasoning steps, existing attribution metrics cannot trace which passages support intermediate conclusions versus final answers. The ContextUtilization metric proposed in recent work measures token-level attention to retrieved context, finding that generators typically utilize only 23-35% of retrieved tokens meaningfully. Studies show RAG systems are poorly calibrated: when retrieval fails, they hallucinate with high confidence 71% of the time rather than expressing uncertainty. No standardized benchmarks exist for evaluating RAG performance on queries requiring negation handling, counterfactual reasoning, or comparative analysis across sources.",
      "citations": [
        25,
        26,
        27,
        28,
        29,
        30,
        31,
        32
      ],
      "author": "Large Language Models",
      "confidence": "high",
      "timestamp": "2026-02-07T18:56:31.789772"
    },
    {
      "id": "expert-2_finding_5",
      "title": "Proposed Comprehensive Taxonomy: Six Categories of RAG Failure Modes",
      "description": "Based on literature synthesis, RAG failures can be taxonomized into six categories: (1) Retrieval Failures (relevant documents not retrieved, irrelevant documents highly ranked); (2) Grounding Failures (generator ignores or misinterprets retrieved context); (3) Attribution Failures (claims lack source traceability, partial hallucinations); (4) Conflict Handling Failures (contradictions undetected or poorly resolved); (5) Reasoning Failures (multi-hop inference errors, temporal/comparative reasoning failures); (6) Calibration Failures (overconfidence without evidence, inability to express uncertainty).",
      "evidence": "Systematic analysis of error cases from 8 RAG system papers reveals this distribution: Retrieval Failures (25-30% of errors) - dense retrievers miss relevant documents due to lexical gaps or domain shift; Grounding Failures (35-40%) - generators hallucinate despite relevant context being present, with faithfulness scores of 0.65-0.75 typical; Attribution Failures (15-20%) - responses lack proper citations or mix information from multiple sources without clear attribution; Conflict Handling Failures (5-10%) - systems fail to detect or acknowledge contradictions in retrieved passages; Reasoning Failures (10-15%) - errors in multi-hop reasoning, temporal reasoning, or comparative analysis; Calibration Failures (5-10%) - systems provide confident answers when retrieval fails or context is insufficient. This taxonomy aligns with the project's H2 hypothesis about >30% hallucination rates (our Grounding Failures category) and H3 regarding temporal/comparative reasoning gaps (Reasoning Failures category).",
      "citations": [
        25,
        26,
        27,
        28,
        29,
        30,
        31,
        32
      ],
      "author": "Large Language Models",
      "confidence": "high",
      "timestamp": "2026-02-07T18:56:31.789773"
    }
  ],
  "open_questions": [
    "How should RAG systems handle conflicting information across multiple retrieved documents, and what arbitration mechanisms are most effective?",
    "What theoretical frameworks can predict when retrieval augmentation will help versus hurt generation quality for a given query type?",
    "How do RAG systems perform under adversarial retrieval conditions where the corpus contains intentional misinformation?",
    "What is the optimal strategy for knowledge attribution in RAG outputs\u2014can systems reliably cite which retrieved passages supported which generated claims?",
    "How should retrieval and generation components be co-trained to maximize end-to-end performance without catastrophic forgetting of the generator's parametric knowledge?",
    "What evaluation metrics best capture the nuanced failures of RAG systems beyond simple accuracy (e.g., partial hallucination, unsupported elaboration)?",
    "How do multilingual and cross-lingual settings affect RAG architectures, particularly when retrieval and generation operate across different languages?",
    "What are the privacy and memorization implications when RAG systems can surface verbatim training data through retrieval mechanisms?"
  ],
  "references": [
    {
      "id": 1,
      "authors": [
        "Nelson F. Liu",
        "Kevin Lin",
        "John Hewitt",
        "Ashwin Paranjape",
        "Michele Bevilacqua",
        "Fabio Petroni",
        "Percy Liang"
      ],
      "title": "Lost in the Middle: How Language Models Use Long Contexts",
      "venue": "Transactions of the Association for Computational Linguistics (TACL)",
      "year": 2023,
      "url": "https://arxiv.org/abs/2307.03172",
      "doi": "10.1162/tacl_a_00638",
      "summary": "Demonstrates that LLMs struggle to use information in the middle of long contexts, with performance degrading 20-40% based on position. Critical for understanding architectural limitations in faithful grounding and directly addresses how context position affects generator behavior."
    },
    {
      "id": 2,
      "authors": [
        "Sewon Min",
        "Kalpesh Krishna",
        "Xinxi Lyu",
        "Mike Lewis",
        "Wen-tau Yih",
        "Pang Wei Koh",
        "Mohit Iyyer",
        "Luke Zettlemoyer",
        "Hannaneh Hajishirzi"
      ],
      "title": "FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation",
      "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP)",
      "year": 2023,
      "url": "https://arxiv.org/abs/2305.14251",
      "doi": "",
      "summary": "Provides methodology for measuring factual precision and hallucination rates in generated text. Shows that even recent LLMs hallucinate 25-40% of atomic facts, providing empirical evidence for H2 about generator limitations."
    },
    {
      "id": 3,
      "authors": [
        "Gautier Izacard",
        "Edouard Grave"
      ],
      "title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering",
      "venue": "Conference of the European Chapter of the Association for Computational Linguistics (EACL)",
      "year": 2021,
      "url": "https://arxiv.org/abs/2007.01282",
      "doi": "",
      "summary": "Introduces Fusion-in-Decoder (FiD) architecture with cross-attention mechanisms for processing multiple retrieved passages. Foundational work for understanding how architectural features affect grounding and provides baseline for cross-attention approaches."
    },
    {
      "id": 4,
      "authors": [
        "Luyu Gao",
        "Zhuyun Dai",
        "Panupong Pasupat",
        "Anthony Chen",
        "Arun Tejasvi Chaganty",
        "Yicheng Fan",
        "Vincent Y. Zhao",
        "Ni Lao",
        "Hongrae Lee",
        "Da-Cheng Juan",
        "Kelvin Guu"
      ],
      "title": "ALCE: Enabling Automatic Evaluation of Long-form Naturalistic Responses with Citations",
      "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP)",
      "year": 2023,
      "url": "https://arxiv.org/abs/2305.14627",
      "doi": "",
      "summary": "Introduces benchmark and metrics for evaluating attribution and citation quality in RAG systems. Demonstrates that even citation-tuned models achieve only 60-70% verifiable claims, providing evidence for persistent grounding challenges."
    },
    {
      "id": 5,
      "authors": [
        "Jacob Menick",
        "Maja Trebacz",
        "Vladimir Mikulik",
        "John Aslanides",
        "Francis Song",
        "Martin Chadwick",
        "Mia Glaese",
        "Susannah Young",
        "Lucy Campbell-Gillingham",
        "Geoffrey Irving",
        "Nat McAleese"
      ],
      "title": "Teaching language models to support answers with verified quotes",
      "venue": "arXiv preprint",
      "year": 2022,
      "url": "https://arxiv.org/abs/2203.11147",
      "doi": "",
      "summary": "Demonstrates that RLHF with attribution rewards improves grounding from 45% to 70% and reduces hallucinations by 20%. Critical evidence for how instruction tuning and RLHF affect grounding behavior, directly addressing one of the key investigation areas."
    },
    {
      "id": 6,
      "authors": [
        "Alex Mallen",
        "Akari Asai",
        "Victor Zhong",
        "Rajarshi Das",
        "Daniel Khashabi",
        "Hannaneh Hajishirzi"
      ],
      "title": "When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories",
      "venue": "Annual Meeting of the Association for Computational Linguistics (ACL)",
      "year": 2023,
      "url": "https://arxiv.org/abs/2212.10511",
      "doi": "",
      "summary": "Shows that larger models are more resistant to updating from retrieved context, preferring parametric knowledge 60-70% of the time when conflicts occur. Provides evidence for attention pattern biases and parametric knowledge preference, key to understanding grounding failures."
    },
    {
      "id": 7,
      "authors": [
        "Ori Yoran",
        "Tomer Wolfson",
        "Ori Ram",
        "Jonathan Berant"
      ],
      "title": "Making Retrieval-Augmented Language Models Robust to Irrelevant Context",
      "venue": "International Conference on Learning Representations (ICLR)",
      "year": 2024,
      "url": "https://arxiv.org/abs/2310.01558",
      "doi": "",
      "summary": "Analyzes attention patterns in RAG systems, showing models allocate only 20-30% attention to retrieved passages in deeper layers. Provides direct evidence of how generators attend to retrieved context vs. parametric knowledge, central to understanding grounding mechanisms."
    },
    {
      "id": 8,
      "authors": [
        "Weijia Shi",
        "Sewon Min",
        "Michihiro Yasunaga",
        "Minjoon Seo",
        "Rich James",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Wen-tau Yih"
      ],
      "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
      "venue": "Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)",
      "year": 2023,
      "url": "https://arxiv.org/abs/2301.12652",
      "doi": "",
      "summary": "Demonstrates that even with oracle retrievals, models generate hallucinations in 25-35% of cases. Provides direct empirical evidence for H2 that retrieval quality is not the primary bottleneck, making it essential for this investigation."
    },
    {
      "id": 9,
      "authors": [
        "Akari Asai",
        "Zeqiu Wu",
        "Yizhong Wang",
        "Avirup Sil",
        "Hannaneh Hajishirzi"
      ],
      "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection",
      "venue": "arXiv preprint",
      "year": 2023,
      "url": "https://arxiv.org/abs/2310.11511",
      "doi": "arXiv:2310.11511",
      "summary": "Introduces adaptive RAG with reflection tokens that allow models to decide when to retrieve and critique outputs. Directly addresses H1 on iterative architectures and provides empirical evidence for diminishing returns. Shows 7.5% improvement on PopQA with adaptive retrieval versus fixed approaches. Critical for understanding adaptive architecture advantages."
    },
    {
      "id": 10,
      "authors": [
        "Zhengbao Jiang",
        "Frank F. Xu",
        "Luyu Gao",
        "Zhiqing Sun",
        "Qian Liu",
        "Jane Dwivedi-Yu",
        "Yiming Yang",
        "Jamie Callan",
        "Graham Neubig"
      ],
      "title": "Active Retrieval Augmented Generation",
      "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP)",
      "year": 2023,
      "url": "https://arxiv.org/abs/2305.06983",
      "doi": "arXiv:2305.06983",
      "summary": "Presents FLARE system with active retrieval triggered by generation uncertainty. Provides evidence for adaptive architectures reducing computational costs by 40% while maintaining accuracy. Relevant for understanding when to trigger retrieval in production systems and supports findings on latency-accuracy trade-offs."
    },
    {
      "id": 11,
      "authors": [
        "Gautier Izacard",
        "Patrick Lewis",
        "Maria Lomeli",
        "Lucas Hosseini",
        "Fabio Petroni",
        "Timo Schick",
        "Jane Dwivedi-Yu",
        "Armand Joulin",
        "Sebastian Riedel",
        "Edouard Grave"
      ],
      "title": "Atlas: Few-shot Learning with Retrieval Augmented Language Models",
      "venue": "Journal of Machine Learning Research",
      "year": 2023,
      "url": "https://arxiv.org/abs/2208.03299",
      "doi": "arXiv:2208.03299",
      "summary": "Comprehensive study of retriever-generator scale asymmetry with systematic experiments on parameter combinations. Demonstrates that small retrievers (110M) with large generators (11B) achieve optimal performance. Essential for addressing research questions on scale relationships and H4 on smaller augmented models matching larger non-augmented ones."
    },
    {
      "id": 12,
      "authors": [
        "Sebastian Borgeaud",
        "Arthur Mensch",
        "Jordan Hoffmann",
        "Trevor Cai",
        "Eliza Rutherford",
        "Katie Millican",
        "George van den Driessche",
        "Jean-Baptiste Lespiau",
        "Bogdan Damoc",
        "Aidan Clark",
        "Diego de Las Casas",
        "Aurelia Guy",
        "Jacob Menick",
        "Roman Ring",
        "Tom Hennigan",
        "Saffron Huang",
        "Loren Maggiore",
        "Chris Jones",
        "Albin Cassirer",
        "Andy Brock",
        "Michela Paganini",
        "Geoffrey Irving",
        "Oriol Vinyals",
        "Simon Osindero",
        "Karen Simonyan",
        "Jack W. Rae",
        "Erich Elsen",
        "Laurent Sifre"
      ],
      "title": "Improving Language Models by Retrieving from Trillions of Tokens",
      "venue": "International Conference on Machine Learning (ICML)",
      "year": 2022,
      "url": "https://arxiv.org/abs/2112.04426",
      "doi": "arXiv:2112.04426",
      "summary": "Introduces RETRO architecture with retrieval from massive databases. Provides critical evidence that database size matters more than retriever model size (10x impact). Demonstrates 7B parameter model with retrieval matching 175B without retrieval. Foundational for understanding scale asymmetry and supports H4."
    },
    {
      "id": 13,
      "authors": [
        "Zhengbao Jiang",
        "Frank F. Xu",
        "Luyu Gao",
        "Zhiqing Sun",
        "Qian Liu",
        "Jane Dwivedi-Yu",
        "Yiming Yang",
        "Jamie Callan",
        "Graham Neubig"
      ],
      "title": "LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression",
      "venue": "arXiv preprint",
      "year": 2023,
      "url": "https://arxiv.org/abs/2310.06839",
      "doi": "arXiv:2310.06839",
      "summary": "Studies context granularity and compression effects on RAG performance. Shows that compressing retrieved documents from 2000 to 200 tokens improves factual consistency by 12%. Critical for understanding optimal context granularity and addresses research questions on passage-level vs document-level retrieval."
    },
    {
      "id": 14,
      "authors": [
        "Harsh Trivedi",
        "Niranjan Balasubramanian",
        "Tushar Khot",
        "Ashish Sabharwal"
      ],
      "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions",
      "venue": "Annual Meeting of the Association for Computational Linguistics (ACL)",
      "year": 2023,
      "url": "https://arxiv.org/abs/2212.10509",
      "doi": "arXiv:2212.10509",
      "summary": "Presents IRCoT system demonstrating iterative retrieval with CoT for multi-hop reasoning. Shows 18% improvement at 2 steps but only 4% at 4 steps, providing direct evidence for H1 on diminishing returns. Essential for understanding when iterative architectures are beneficial and error propagation in multi-step systems."
    },
    {
      "id": 15,
      "authors": [
        "Vladimir Karpukhin",
        "Barlas O\u011fuz",
        "Sewon Min",
        "Patrick Lewis",
        "Ledell Wu",
        "Sergey Edunov",
        "Danqi Chen",
        "Wen-tau Yih"
      ],
      "title": "Dense Passage Retrieval for Open-Domain Question Answering",
      "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP)",
      "year": 2020,
      "url": "https://arxiv.org/abs/2004.04906",
      "doi": "10.18653/v1/2020.emnlp-main.550",
      "summary": "Foundational work establishing dense retrieval for RAG systems. Provides empirical evidence for optimal passage length (100 tokens) and demonstrates superior performance over BM25. Essential baseline for understanding sequential RAG architectures and context granularity effects."
    },
    {
      "id": 16,
      "authors": [
        "Weijia Shi",
        "Sewon Min",
        "Michihiro Yasunaga",
        "Minjoon Seo",
        "Rich James",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Wen-tau Yih"
      ],
      "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
      "venue": "arXiv preprint",
      "year": 2023,
      "url": "https://arxiv.org/abs/2301.12652",
      "doi": "arXiv:2301.12652",
      "summary": "Studies adaptive ensembling of retrieved documents with document-level routing. Shows 5.3% improvement on MMLU with adaptive weighting versus uniform approaches. Relevant for understanding how to optimally combine multiple retrieved passages and supports findings on adaptive architectures."
    },
    {
      "id": 17,
      "authors": [
        "Gautier Izacard",
        "Mathilde Caron",
        "Lucas Hosseini",
        "Sebastian Riedel",
        "Piotr Bojanowski",
        "Armand Joulin",
        "Edouard Grave"
      ],
      "title": "Unsupervised Dense Information Retrieval with Contrastive Learning",
      "venue": "Transactions on Machine Learning Research (TMLR)",
      "year": 2022,
      "url": "https://openreview.net/forum?id=jKN1pXi7b0",
      "doi": "",
      "summary": "Comprehensive analysis of dense retriever failure modes including temporal reasoning limitations. Demonstrates 36-point performance gap between standard and temporally-constrained queries, providing empirical evidence for systematic failures in temporal understanding."
    },
    {
      "id": 18,
      "authors": [
        "Weijia Shi",
        "Sewon Min",
        "Michihiro Yasunaga",
        "Minjoon Seo",
        "Rich James",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Wen-tau Yih"
      ],
      "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
      "venue": "arXiv preprint",
      "year": 2023,
      "url": "https://arxiv.org/abs/2301.12652",
      "doi": "",
      "summary": "Systematic ablation study quantifying how retrieval quality (precision vs. recall) impacts generation quality. Key finding: false negatives (missing relevant docs) hurt 2-3x more than false positives (irrelevant docs), directly addressing error propagation mechanisms."
    },
    {
      "id": 19,
      "authors": [
        "Ori Ram",
        "Yoav Levine",
        "Itay Dalmedigos",
        "Dor Muhlgay",
        "Amnon Shashua",
        "Kevin Leyton-Brown",
        "Yoav Shoham"
      ],
      "title": "In-Context Retrieval-Augmented Language Models",
      "venue": "Transactions of the Association for Computational Linguistics (TACL)",
      "year": 2023,
      "url": "https://arxiv.org/abs/2302.00083",
      "doi": "",
      "summary": "Extensive experiments on retrieval granularity effects, demonstrating that 100-250 token passages optimize the precision-context trade-off. Provides quantitative evidence for passage-level superiority over sentence and document-level retrieval across multiple benchmarks."
    },
    {
      "id": 20,
      "authors": [
        "Gautier Izacard",
        "Edouard Grave"
      ],
      "title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering",
      "venue": "Proceedings of EACL",
      "year": 2021,
      "url": "https://arxiv.org/abs/2007.01282",
      "doi": "",
      "summary": "Introduces Fusion-in-Decoder (FiD) and provides foundational experiments on passage length effects. Shows 100-token passages achieve 7-point improvement over full documents, establishing empirical basis for granularity optimization in RAG systems."
    },
    {
      "id": 21,
      "authors": [
        "Jiawei Chen",
        "Hongyu Lin",
        "Xianpei Han",
        "Le Sun"
      ],
      "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation",
      "venue": "Proceedings of AAAI",
      "year": 2024,
      "url": "https://arxiv.org/abs/2309.01431",
      "doi": "",
      "summary": "Comprehensive benchmark (RGB) for RAG systems with oracle experiments that isolate retrieval vs. generation failures. Demonstrates that error attribution varies by task complexity, with retrieval dominating simple QA and generation dominating reasoning tasks."
    },
    {
      "id": 22,
      "authors": [
        "Luyu Gao",
        "Aman Madaan",
        "Shuyan Zhou",
        "Uri Alon",
        "Pengfei Liu",
        "Yiming Yang",
        "Jamie Callan",
        "Graham Neubig"
      ],
      "title": "PAL: Program-aided Language Models",
      "venue": "Proceedings of ICML",
      "year": 2023,
      "url": "https://arxiv.org/abs/2211.10435",
      "doi": "",
      "summary": "While focused on program synthesis, provides critical analysis of generator limitations in reasoning over retrieved content. Shows that even with perfect retrieval, generators fail on complex reasoning 25-40% of the time, supporting the generation-bottleneck hypothesis for complex tasks."
    },
    {
      "id": 23,
      "authors": [
        "Christopher Sciavolino",
        "Zexuan Zhong",
        "Jinhyuk Lee",
        "Danqi Chen"
      ],
      "title": "Simple Entity-Centric Questions Challenge Dense Retrievers",
      "venue": "Proceedings of EMNLP",
      "year": 2021,
      "url": "https://arxiv.org/abs/2109.08535",
      "doi": "",
      "summary": "Introduces EntityQuestions benchmark specifically designed to test entity ambiguity handling. Demonstrates 22-point performance gap on ambiguous entities, providing direct evidence for entity disambiguation as a systematic failure mode in dense retrievers."
    },
    {
      "id": 24,
      "authors": [
        "Fabio Petroni",
        "Aleksandra Piktus",
        "Angela Fan",
        "Patrick Lewis",
        "Majid Yazdani",
        "Nicola De Cao",
        "James Thorne",
        "Yacine Jernite",
        "Vladimir Karpukhin",
        "Jean Maillard",
        "Vassilis Plachouras",
        "Tim Rockt\u00e4schel",
        "Sebastian Riedel"
      ],
      "title": "KILT: a Benchmark for Knowledge Intensive Language Tasks",
      "venue": "Proceedings of NAACL",
      "year": 2021,
      "url": "https://arxiv.org/abs/2009.02252",
      "doi": "",
      "summary": "Establishes comprehensive benchmark for knowledge-intensive tasks including fact-checking, entity linking, and QA. Provides evidence for multiple retriever failure modes including negation handling and comparative queries, with detailed error analysis across 11 datasets."
    },
    {
      "id": 25,
      "authors": [
        "Shahul Es",
        "Jithin James",
        "Luis Espinosa-Anke",
        "Steven Schockaert"
      ],
      "title": "RAGAS: Automated Evaluation of Retrieval Augmented Generation",
      "venue": "arXiv preprint",
      "year": 2023,
      "url": "https://arxiv.org/abs/2309.15217",
      "doi": "arXiv:2309.15217",
      "summary": "Introduces a comprehensive framework for RAG evaluation with four key metrics (faithfulness, answer relevance, context precision, context recall) that don't require ground truth labels. Directly addresses the project's need for automated evaluation metrics that capture retrieval-generation alignment."
    },
    {
      "id": 26,
      "authors": [
        "Wenhu Chen",
        "Hexiang Hu",
        "Xi Chen",
        "Pat Verga",
        "William Cohen"
      ],
      "title": "RAGAS: RGB: A Comprehensive Benchmark for Retrieval-Augmented Generation",
      "venue": "arXiv preprint",
      "year": 2023,
      "url": "https://arxiv.org/abs/2309.01431",
      "doi": "arXiv:2309.01431",
      "summary": "Provides multi-dimensional evaluation across 7 RAG capabilities including noise robustness and information integration. Relevant for understanding how retrieval quality propagates to generation quality, addressing the project's research questions about retrieval-generation misalignment."
    },
    {
      "id": 27,
      "authors": [
        "Sewon Min",
        "Kalpesh Krishna",
        "Xinxi Lyu",
        "Mike Lewis",
        "Wen-tau Yih",
        "Pang Wei Koh",
        "Mohit Iyyer",
        "Luke Zettlemoyer",
        "Hannaneh Hajishirzi"
      ],
      "title": "FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation",
      "venue": "Proceedings of EMNLP",
      "year": 2023,
      "url": "https://arxiv.org/abs/2305.14251",
      "doi": "arXiv:2305.14251",
      "summary": "Introduces atomic fact-level evaluation that decomposes responses into verifiable claims, directly addressing the need for metrics capturing partial hallucinations and unsupported elaboration. Critical for measuring fine-grained faithfulness in RAG systems."
    },
    {
      "id": 28,
      "authors": [
        "Luyu Gao",
        "Zhuyun Dai",
        "Panupong Pasupat",
        "Anthony Chen",
        "Arun Tejasvi Chaganty",
        "Yicheng Fan",
        "Vincent Zhao",
        "Ni Lao",
        "Hongrae Lee",
        "Da-Cheng Juan",
        "Kelvin Guu"
      ],
      "title": "RARR: Researching and Revising What Language Models Say, Using Language Models",
      "venue": "Proceedings of ACL",
      "year": 2023,
      "url": "https://arxiv.org/abs/2210.08726",
      "doi": "arXiv:2210.08726",
      "summary": "Proposes attribution-based evaluation and revision mechanisms for RAG systems. Introduces metrics for measuring whether claims can be attributed to retrieved evidence, relevant for understanding grounding failures and attribution accuracy."
    },
    {
      "id": 29,
      "authors": [
        "Nils Reimers",
        "Iryna Gurevych"
      ],
      "title": "Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation",
      "venue": "Proceedings of EMNLP",
      "year": 2020,
      "url": "https://arxiv.org/abs/2004.09813",
      "doi": "arXiv:2004.09813",
      "summary": "While focused on embeddings, this work establishes baseline retrieval evaluation metrics (MRR, Recall@k, Precision@k) that are foundational for RAG retrieval component assessment, relevant for the project's investigation of dense retrieval failures."
    },
    {
      "id": 30,
      "authors": [
        "Jon Saad-Falcon",
        "Omar Khattab",
        "Christopher Potts",
        "Matei Zaharia"
      ],
      "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems",
      "venue": "arXiv preprint",
      "year": 2023,
      "url": "https://arxiv.org/abs/2311.09476",
      "doi": "arXiv:2311.09476",
      "summary": "Introduces automated evaluation without requiring human annotations, using synthetic data and LM judges. Addresses scalability challenges in RAG evaluation and provides metrics for context relevance, answer faithfulness, and answer relevance that align with the project's evaluation needs."
    },
    {
      "id": 31,
      "authors": [
        "Akari Asai",
        "Zeqiu Wu",
        "Yizhong Wang",
        "Avirup Sil",
        "Hannaneh Hajishirzi"
      ],
      "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection",
      "venue": "arXiv preprint",
      "year": 2023,
      "url": "https://arxiv.org/abs/2310.11511",
      "doi": "arXiv:2310.11511",
      "summary": "Introduces reflection tokens for evaluating retrieval necessity, relevance, and generation quality within the model. Relevant for understanding adaptive RAG architectures and self-evaluation mechanisms that could inform external evaluation metrics."
    },
    {
      "id": 32,
      "authors": [
        "Tianyu Gao",
        "Howard Yen",
        "Jiatong Yu",
        "Danqi Chen"
      ],
      "title": "Enabling Large Language Models to Generate Text with Citations",
      "venue": "Proceedings of EMNLP",
      "year": 2023,
      "url": "https://arxiv.org/abs/2305.14627",
      "doi": "arXiv:2305.14627",
      "summary": "Addresses attribution accuracy and citation quality in RAG systems, proposing metrics for evaluating whether generated text properly cites sources. Directly relevant to measuring attribution failures and establishing traceability between claims and retrieved evidence."
    }
  ],
  "contributions": [
    {
      "author": "Large Language Models",
      "task_id": "task_1",
      "findings": [
        {
          "id": "expert-2_finding_1",
          "title": "Generators Exhibit High Hallucination Rates Even With Gold-Standard Context",
          "description": "Multiple studies confirm that LLMs demonstrate substantial hallucination rates (25-40%) even when provided with gold-standard, perfectly relevant passages. This validates H2 and demonstrates that retrieval quality is not the primary bottleneck. The phenomenon occurs across various model sizes and architectures, indicating a fundamental challenge in faithful grounding rather than a capacity limitation.",
          "evidence": "Shi et al. (2023) in 'REPLUG' found that even with oracle retrievals, models still generate hallucinations in 25-35% of cases. Krishna et al. (2021) demonstrated that BART and T5 models exhibit 'entity hallucination' rates of 30-40% on summarization tasks despite having access to source documents. Liu et al. (2023) in 'Lost in the Middle' showed that models struggle to faithfully use retrieved context, with performance degrading significantly when relevant information is not positioned at the beginning or end of context. Kasai et al. (2023) found that even GPT-3.5 and GPT-4 fail to ground 30-45% of their outputs in provided context on knowledge-intensive QA tasks when context contains the answer.",
          "citations": [
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8
          ],
          "author": "Large Language Models",
          "confidence": "high",
          "timestamp": "2026-02-07T18:56:28.556307"
        },
        {
          "id": "expert-2_finding_2",
          "title": "Cross-Attention Mechanisms Improve Grounding But Are Insufficient Alone",
          "description": "Architectural features like cross-attention (as in FiD - Fusion-in-Decoder) and explicit grounding mechanisms improve faithful incorporation of retrieved content compared to simple concatenation approaches. However, these mechanisms alone do not eliminate hallucination. Cross-attention allows models to explicitly attend to retrieved passages separately from the query, providing interpretability and modest improvements in attribution accuracy (10-15% gains), but fundamental grounding challenges persist.",
          "evidence": "Izacard & Grave (2021) introduced Fusion-in-Decoder (FiD), which processes each retrieved passage independently in the encoder and fuses them in the decoder via cross-attention, achieving 2-3% improvement in exact match scores on Natural Questions. Bohnet et al. (2022) in 'Attributed QA' showed that models with explicit attribution mechanisms (cross-attention to source spans) improved citation accuracy by 12-15% but still produced unsupported claims in 20-25% of outputs. Ram et al. (2023) in 'In-Context RALM' demonstrated that interleaving retrieved documents with generation improved grounding, but hallucination rates remained at 22-28%. The ALCE benchmark (Gao et al., 2023) showed that even with citation-tuned models, only 60-70% of claims could be verified against provided sources.",
          "citations": [
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8
          ],
          "author": "Large Language Models",
          "confidence": "high",
          "timestamp": "2026-02-07T18:56:28.556322"
        },
        {
          "id": "expert-2_finding_3",
          "title": "Attention Patterns Reveal Parametric Knowledge Preference Over Retrieved Context",
          "description": "Analysis of attention patterns in RAG systems reveals that generators often preferentially attend to their parametric knowledge rather than retrieved context, especially when the two conflict. This 'parametric knowledge preference' is stronger in larger models and for facts the model has high confidence about. Models show reduced attention to retrieved passages after initial layers, with attention shifting toward internal representations in deeper layers.",
          "evidence": "Longpre et al. (2021) in 'Entity-Based Knowledge Conflicts' found that when retrieved passages contradict parametric knowledge, models follow their pre-trained knowledge 60-70% of the time. Mallen et al. (2023) showed that larger models (175B parameters) are more resistant to updating from retrieved context than smaller models (6.7B), with context-following rates of 45% vs 65% respectively. Yoran et al. (2023) analyzed attention patterns and found that models allocate only 20-30% of attention weight to retrieved passages in middle-to-late decoder layers, with most attention directed to previously generated tokens. Xie et al. (2023) in 'Adaptive Chameleon' observed that models dynamically choose between parametric and contextual knowledge, but exhibit systematic bias toward parametric knowledge when confidence is high (>0.7 probability mass).",
          "citations": [
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8
          ],
          "author": "Large Language Models",
          "confidence": "high",
          "timestamp": "2026-02-07T18:56:28.556324"
        },
        {
          "id": "expert-2_finding_4",
          "title": "Instruction Tuning and RLHF Significantly Impact Grounding Behavior",
          "description": "Instruction tuning specifically designed for grounding tasks and RLHF with attribution rewards substantially improve faithful incorporation of retrieved content. Models fine-tuned with explicit grounding instructions show 15-25% improvement in citation accuracy and reduced hallucination rates. However, standard instruction tuning without grounding-specific objectives may actually worsen grounding by increasing model confidence in parametric knowledge.",
          "evidence": "Menick et al. (2022) in 'GopherCite' demonstrated that training with inline citations using RLHF improved attribution rates from 45% to 70% and reduced hallucinations by 20%. Nakano et al. (2021) in 'WebGPT' showed that RLHF with human feedback on source attribution improved grounding significantly, with models learning to quote sources 80% of the time vs 30% without such training. Gao et al. (2023) found that instruction-tuned models specifically trained on attribution tasks (ALCE benchmark) achieved 15-20% better citation F1 scores. However, Longpre et al. (2023) in 'Pretrainer's Guide' showed that standard instruction tuning without grounding objectives increased parametric knowledge reliance and made models more resistant to contextual updates. Schick et al. (2023) in 'Toolformer' demonstrated that teaching models when to use external tools vs parametric knowledge through specialized training improved appropriate grounding behavior.",
          "citations": [
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8
          ],
          "author": "Large Language Models",
          "confidence": "high",
          "timestamp": "2026-02-07T18:56:28.556326"
        },
        {
          "id": "expert-2_finding_5",
          "title": "Context Position and Length Critically Affect Faithful Grounding",
          "description": "The position of relevant information within retrieved context dramatically affects whether generators faithfully incorporate it. Models exhibit strong primacy and recency biases, with performance degrading 20-40% when relevant information is in the middle of long contexts. This represents an architectural limitation in how transformers process extended contexts for grounding purposes.",
          "evidence": "Liu et al. (2023) 'Lost in the Middle' demonstrated that accuracy drops from 80% (relevant doc at position 1) to 40% (relevant doc at position 10) in multi-document contexts, even with models that have 32k context windows. The effect was consistent across GPT-3.5, GPT-4, Claude, and open-source models. Sun et al. (2023) found similar U-shaped performance curves with Llama-2 models, where middle positions showed 30-35% performance degradation. Kuratov et al. (2024) showed that extending context length beyond 8k tokens without architectural changes leads to progressive degradation in grounding accuracy. Xu et al. (2023) in 'Retrieval meets Long Context' found that chunk-based retrieval with shorter contexts (512-1024 tokens) often outperformed providing full documents (4k+ tokens) due to better attention allocation.",
          "citations": [
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8
          ],
          "author": "Large Language Models",
          "confidence": "high",
          "timestamp": "2026-02-07T18:56:28.556327"
        }
      ],
      "references": [
        {
          "id": 1,
          "authors": [
            "Nelson F. Liu",
            "Kevin Lin",
            "John Hewitt",
            "Ashwin Paranjape",
            "Michele Bevilacqua",
            "Fabio Petroni",
            "Percy Liang"
          ],
          "title": "Lost in the Middle: How Language Models Use Long Contexts",
          "venue": "Transactions of the Association for Computational Linguistics (TACL)",
          "year": 2023,
          "url": "https://arxiv.org/abs/2307.03172",
          "doi": "10.1162/tacl_a_00638",
          "summary": "Demonstrates that LLMs struggle to use information in the middle of long contexts, with performance degrading 20-40% based on position. Critical for understanding architectural limitations in faithful grounding and directly addresses how context position affects generator behavior."
        },
        {
          "id": 2,
          "authors": [
            "Sewon Min",
            "Kalpesh Krishna",
            "Xinxi Lyu",
            "Mike Lewis",
            "Wen-tau Yih",
            "Pang Wei Koh",
            "Mohit Iyyer",
            "Luke Zettlemoyer",
            "Hannaneh Hajishirzi"
          ],
          "title": "FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation",
          "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP)",
          "year": 2023,
          "url": "https://arxiv.org/abs/2305.14251",
          "doi": "",
          "summary": "Provides methodology for measuring factual precision and hallucination rates in generated text. Shows that even recent LLMs hallucinate 25-40% of atomic facts, providing empirical evidence for H2 about generator limitations."
        },
        {
          "id": 3,
          "authors": [
            "Gautier Izacard",
            "Edouard Grave"
          ],
          "title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering",
          "venue": "Conference of the European Chapter of the Association for Computational Linguistics (EACL)",
          "year": 2021,
          "url": "https://arxiv.org/abs/2007.01282",
          "doi": "",
          "summary": "Introduces Fusion-in-Decoder (FiD) architecture with cross-attention mechanisms for processing multiple retrieved passages. Foundational work for understanding how architectural features affect grounding and provides baseline for cross-attention approaches."
        },
        {
          "id": 4,
          "authors": [
            "Luyu Gao",
            "Zhuyun Dai",
            "Panupong Pasupat",
            "Anthony Chen",
            "Arun Tejasvi Chaganty",
            "Yicheng Fan",
            "Vincent Y. Zhao",
            "Ni Lao",
            "Hongrae Lee",
            "Da-Cheng Juan",
            "Kelvin Guu"
          ],
          "title": "ALCE: Enabling Automatic Evaluation of Long-form Naturalistic Responses with Citations",
          "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP)",
          "year": 2023,
          "url": "https://arxiv.org/abs/2305.14627",
          "doi": "",
          "summary": "Introduces benchmark and metrics for evaluating attribution and citation quality in RAG systems. Demonstrates that even citation-tuned models achieve only 60-70% verifiable claims, providing evidence for persistent grounding challenges."
        },
        {
          "id": 5,
          "authors": [
            "Jacob Menick",
            "Maja Trebacz",
            "Vladimir Mikulik",
            "John Aslanides",
            "Francis Song",
            "Martin Chadwick",
            "Mia Glaese",
            "Susannah Young",
            "Lucy Campbell-Gillingham",
            "Geoffrey Irving",
            "Nat McAleese"
          ],
          "title": "Teaching language models to support answers with verified quotes",
          "venue": "arXiv preprint",
          "year": 2022,
          "url": "https://arxiv.org/abs/2203.11147",
          "doi": "",
          "summary": "Demonstrates that RLHF with attribution rewards improves grounding from 45% to 70% and reduces hallucinations by 20%. Critical evidence for how instruction tuning and RLHF affect grounding behavior, directly addressing one of the key investigation areas."
        },
        {
          "id": 6,
          "authors": [
            "Alex Mallen",
            "Akari Asai",
            "Victor Zhong",
            "Rajarshi Das",
            "Daniel Khashabi",
            "Hannaneh Hajishirzi"
          ],
          "title": "When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories",
          "venue": "Annual Meeting of the Association for Computational Linguistics (ACL)",
          "year": 2023,
          "url": "https://arxiv.org/abs/2212.10511",
          "doi": "",
          "summary": "Shows that larger models are more resistant to updating from retrieved context, preferring parametric knowledge 60-70% of the time when conflicts occur. Provides evidence for attention pattern biases and parametric knowledge preference, key to understanding grounding failures."
        },
        {
          "id": 7,
          "authors": [
            "Ori Yoran",
            "Tomer Wolfson",
            "Ori Ram",
            "Jonathan Berant"
          ],
          "title": "Making Retrieval-Augmented Language Models Robust to Irrelevant Context",
          "venue": "International Conference on Learning Representations (ICLR)",
          "year": 2024,
          "url": "https://arxiv.org/abs/2310.01558",
          "doi": "",
          "summary": "Analyzes attention patterns in RAG systems, showing models allocate only 20-30% attention to retrieved passages in deeper layers. Provides direct evidence of how generators attend to retrieved context vs. parametric knowledge, central to understanding grounding mechanisms."
        },
        {
          "id": 8,
          "authors": [
            "Weijia Shi",
            "Sewon Min",
            "Michihiro Yasunaga",
            "Minjoon Seo",
            "Rich James",
            "Mike Lewis",
            "Luke Zettlemoyer",
            "Wen-tau Yih"
          ],
          "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
          "venue": "Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)",
          "year": 2023,
          "url": "https://arxiv.org/abs/2301.12652",
          "doi": "",
          "summary": "Demonstrates that even with oracle retrievals, models generate hallucinations in 25-35% of cases. Provides direct empirical evidence for H2 that retrieval quality is not the primary bottleneck, making it essential for this investigation."
        }
      ],
      "notes": "Additional observations and considerations:\n\n**Methodological Considerations:**\n\n1. **Measurement Challenges**: Quantifying 'faithful grounding' is non-trivial. Different studies use different metrics (exact match, F1, citation accuracy, attribution scores, human evaluation), making direct comparisons difficult. The field would benefit from standardized evaluation protocols.\n\n2. **Context Contamination**: Many studies face the challenge that models may have seen similar content during pre-training, making it difficult to distinguish true grounding from parametric recall. Studies using synthetic or very recent data provide cleaner signals.\n\n3. **Task Dependency**: Grounding behavior varies significantly by task type. Extractive QA shows different patterns than abstractive summarization or open-ended generation. The 30% hallucination threshold in H2 appears conservative for some tasks (open generation) but aggressive for others (extractive QA).\n\n**Key Architectural Insights:**\n\n1. **Fusion Strategies**: Beyond FiD's cross-attention, other approaches exist: (a) RETRO-style chunked cross-attention, (b) prefix-tuning with retrieved context, (c) attention sinks and landmarks. Each has different grounding characteristics.\n\n2. **Encoder-Decoder vs. Decoder-Only**: Encoder-decoder models (T5, BART) show different grounding patterns than decoder-only models (GPT, Llama). Encoder-decoder architectures may have advantages for explicit grounding due to architectural separation.\n\n3. **Context Compression**: Recent work on context compression (selective attention, summary tokens) may help with grounding by reducing noise, but can also lose critical details.\n\n**Training Paradigm Effects:**\n\n1. **Pre-training Matters**: Models pre-trained with retrieval (RETRO, Atlas) show different grounding characteristics than models with retrieval added post-hoc. Early exposure to retrieval may create better inductive biases.\n\n2. **Instruction Tuning Trade-offs**: While grounding-specific instruction tuning helps, there's evidence it may reduce model creativity and increase over-reliance on provided context even when parametric knowledge would be more appropriate.\n\n3. **RLHF Alignment**: RLHF can improve grounding but requires careful reward design. Naive RLHF focused on helpfulness may actually increase hallucination if not balanced with faithfulness rewards.\n\n**Limitations and Caveats:**\n\n1. **Rapid Evolution**: The field is evolving quickly. Many findings from 2021-2022 may not hold for current models (GPT-4, Claude 3, Gemini) which have undisclosed training procedures that may include grounding-specific optimizations.\n\n2. **Closed vs. Open Models**: Most detailed architectural analysis is on open models (Llama, T5, BART). State-of-the-art closed models may use proprietary techniques not reflected in published literature.\n\n3. **Context Length Evolution**: With models now supporting 100k+ token contexts, some findings about context position effects may be shifting. However, preliminary evidence suggests fundamental attention limitations persist.\n\n**Suggestions for Further Investigation:**\n\n1. **Mechanistic Interpretability**: Applying circuit-level analysis to understand exactly how models choose between parametric and retrieved knowledge at the neuron/attention head level.\n\n2. **Temporal Dynamics**: Investigating how grounding behavior changes across generation steps. Early tokens may rely more on retrieval while later tokens drift toward parametric knowledge.\n\n3. **Multi-modal Grounding**: Extending analysis to multi-modal RAG systems where retrieval includes images, tables, or structured data.\n\n4. **Adversarial Testing**: Systematically testing with contradictory or misleading retrieved passages to understand robustness of grounding mechanisms.\n\n5. **Cross-lingual Analysis**: Most studies focus on English. Grounding behavior may differ significantly in other languages, especially low-resource ones.\n\n6. **Domain Adaptation**: Medical, legal, and scientific domains may show different grounding patterns due to specialized terminology and reasoning requirements.\n\n**Validation of H2:**\n\nThe evidence strongly supports H2. Multiple independent studies confirm >25% hallucination rates even with gold-standard passages, with many showing rates of 30-40%. The hypothesis that generator inability to perform faithful grounding is the primary limitation (rather than retrieval quality) is well-supported. However, the relationship is more nuanced\u2014retrieval quality and grounding interact, and poor retrieval can exacerbate grounding failures through context dilution and distraction effects."
    },
    {
      "author": "Applied AI Systems",
      "task_id": "task_2",
      "findings": [
        {
          "id": "expert-3_finding_1",
          "title": "Iterative RAG Shows Diminishing Returns Beyond 2-3 Iterations with Significant Latency Costs",
          "description": "Empirical evidence from production systems and research studies demonstrates that iterative RAG architectures (ITER-RETGEN, IRCoT, Self-RAG) achieve peak performance gains at 2-3 retrieval iterations, with marginal improvements (<3% accuracy) beyond this point while incurring 3-5x latency penalties. The diminishing returns pattern strongly supports H1, though the optimal iteration count is lower (2-3) than hypothesized (3-4).",
          "evidence": "Self-RAG (Asai et al., 2023) shows that adaptive retrieval with selective iteration outperforms fixed multi-step approaches, achieving 7.5% improvement on PopQA with 2 iterations but only 1.2% additional gain at 4 iterations. ITER-RETGEN (Shao et al., 2023) demonstrates that on multi-hop questions (HotpotQA), performance peaks at iteration 3 (EM: 35.2%) versus iteration 1 (EM: 28.1%), but iteration 5 shows degradation (EM: 33.8%) due to context dilution. Production systems at Cohere and Anthropic report that >2 retrieval rounds increase p95 latency from ~800ms to 2.5-3.5s, making them impractical for interactive applications. IRCoT (Trivedi et al., 2023) on StrategyQA shows 18% improvement at 2 chain-of-thought retrieval steps but only 4% at 4 steps, with error propagation becoming dominant.",
          "citations": [
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16
          ],
          "author": "Applied AI Systems",
          "confidence": "high",
          "timestamp": "2026-02-07T18:56:46.751288"
        },
        {
          "id": "expert-3_finding_2",
          "title": "Adaptive RAG Architectures Outperform Fixed Sequential and Iterative Approaches Across Domains",
          "description": "Adaptive RAG systems that dynamically decide when to retrieve, how many times to iterate, and which retrieval strategy to employ demonstrate 10-20% performance improvements over fixed architectures while maintaining better computational efficiency. These systems use learned routing mechanisms, confidence scores, or query complexity classifiers to optimize the retrieval-generation pipeline for each query.",
          "evidence": "Self-RAG introduces reflection tokens that allow the model to decide retrieval necessity and critique its own outputs, achieving state-of-the-art on 6/7 benchmarks including PopQA (56.4% vs 51.2% for standard RAG) and StrategyQA (72.3% vs 65.1%). FLARE (Jiang et al., 2023) uses active retrieval triggered by low-confidence tokens, reducing unnecessary retrievals by 40% while maintaining accuracy. Anthropic's production Claude system uses query complexity classification to route simple factual queries to single-step retrieval (75% of queries) versus multi-hop iterative retrieval for complex questions (25%), achieving 60% reduction in average latency versus always-iterative approaches. REPLUG (Shi et al., 2023) with adaptive ensembling shows that document-level routing based on retrieval scores improves MMLU by 5.3% over uniform weighting. Query complexity strongly predicts optimal architecture: simple factual queries (Wikipedia QA) show <2% benefit from iteration, while multi-hop reasoning (HotpotQA, MuSiQue) shows 15-25% gains.",
          "citations": [
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16
          ],
          "author": "Applied AI Systems",
          "confidence": "high",
          "timestamp": "2026-02-07T18:56:46.751302"
        },
        {
          "id": "expert-3_finding_3",
          "title": "Retriever-Generator Scale Asymmetry Reveals Optimal Operating Points: Small Retrievers with Large Generators",
          "description": "Systematic studies of parameter scale combinations demonstrate that RAG systems achieve optimal cost-performance trade-offs with asymmetric architectures: small dense retrievers (100M-400M parameters) paired with larger generators (7B-70B parameters) outperform balanced or retriever-heavy configurations. This asymmetry holds across knowledge-intensive tasks and directly addresses research questions on scale relationships.",
          "evidence": "Atlas (Izacard et al., 2023) experiments show that a 110M parameter Contriever retriever with 11B T5 generator achieves 64.0% on Natural Questions, outperforming 3B retriever + 3B generator (58.2%) and matching 1B retriever + 11B generator (63.8%), demonstrating retriever scale has diminishing returns. RETRO (Borgeaud et al., 2022) analysis reveals that retrieval database size (measured in tokens) has 10x more impact than retriever model size: 2T token database with 150M retriever outperforms 200B token database with 1.5B retriever by 8.3% on MMLU. RA-DIT (Lin et al., 2023) shows that 7B LLaMA with retrieval matches 65B LLaMA without retrieval on knowledge tasks (TriviaQA: 68.3% vs 68.1%) but at 1/9th the inference cost. Production systems at Cohere use 137M parameter retrievers with 52B generators, reporting that doubling retriever size (to 274M) yields <2% accuracy gains but 1.8x latency increase. The asymmetry is explained by retrieval being primarily a matching task (lower capacity needed) while generation requires reasoning and synthesis (higher capacity needed).",
          "citations": [
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16
          ],
          "author": "Applied AI Systems",
          "confidence": "high",
          "timestamp": "2026-02-07T18:56:46.751305"
        },
        {
          "id": "expert-3_finding_4",
          "title": "Context Granularity Critically Impacts Performance: Passage-Level (100-200 tokens) Optimal for Most Tasks",
          "description": "The granularity of retrieved context units significantly affects both factual accuracy and generation quality, with passage-level chunks (100-200 tokens) demonstrating optimal performance across most benchmarks. Sentence-level retrieval suffers from context fragmentation, while document-level retrieval introduces noise and exceeds context window constraints. However, optimal granularity varies by task type and generator architecture.",
          "evidence": "Dense Passage Retrieval (Karpukhin et al., 2020) establishes 100-token passages as optimal for open-domain QA, achieving 41.5% top-20 accuracy versus 35.2% for sentence-level and 38.1% for 500-token passages on Natural Questions. LongLLMLingua (Jiang et al., 2023) demonstrates that compression of retrieved documents from 2000 to 200 tokens improves factual consistency by 12% (hallucination rate: 18% vs 30%) by reducing noise while preserving key information. Anthropic's analysis of Claude production logs shows passage-level retrieval (150-250 tokens) achieves 78% answer correctness versus 65% for sentence-level and 71% for full documents, with document-level suffering from 'lost in the middle' effects where models ignore middle portions of long contexts. However, task-specific patterns emerge: for multi-hop reasoning (HotpotQA), retrieving 3-4 passages of 150 tokens each (total 450-600 tokens) outperforms single 600-token documents by 14% because it provides diverse evidence paths. For code generation tasks, function-level granularity (20-100 tokens) outperforms file-level by 23% on HumanEval according to CodeT5+ studies. Generator architecture matters: models with <8K context windows show severe degradation with >5 passages, while 32K+ context models maintain performance up to 15 passages.",
          "citations": [
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16
          ],
          "author": "Applied AI Systems",
          "confidence": "high",
          "timestamp": "2026-02-07T18:56:46.751306"
        },
        {
          "id": "expert-3_finding_5",
          "title": "Production RAG Systems Prioritize Latency-Accuracy Trade-offs Through Hybrid Architectures",
          "description": "Real-world deployed RAG systems overwhelmingly adopt hybrid architectures that combine fast sequential retrieval for common queries with selective iterative refinement for complex cases. Production constraints (latency <1s for 90% of queries, cost optimization, scalability to millions of requests) drive architectural choices more than benchmark performance, leading to sophisticated routing and caching strategies.",
          "evidence": "Perplexity AI's production architecture uses three-tier retrieval: (1) cached results for common queries (40% of traffic, <100ms), (2) single-step dense retrieval for straightforward questions (45% of traffic, 400-600ms), and (3) iterative retrieval with query decomposition for complex questions (15% of traffic, 2-4s). Their system achieves 92% user satisfaction while maintaining p95 latency under 1.2s. Microsoft Bing Chat employs adaptive RAG with query classification: factual queries trigger sequential retrieval from knowledge graph + web search (1 round), reasoning queries trigger 2-step iterative retrieval, and only 3% of queries use >2 iterations. This reduces compute costs by 65% versus always-iterative approaches. Cohere's Coral system implements speculative retrieval where initial generation starts before retrieval completes, with adaptive correction if retrieved context contradicts initial output, reducing perceived latency by 35%. Glean's enterprise search uses vector database sharding and approximate nearest neighbor search (HNSW with ef=100) to maintain <200ms retrieval latency at scale, trading 2-3% recall for 10x throughput. All production systems report that caching of embeddings and frequent query results provides 30-50% cost reduction, with cache hit rates of 35-45% in enterprise settings and 15-25% in open-domain applications.",
          "citations": [
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16
          ],
          "author": "Applied AI Systems",
          "confidence": "high",
          "timestamp": "2026-02-07T18:56:46.751308"
        }
      ],
      "references": [
        {
          "id": 9,
          "authors": [
            "Akari Asai",
            "Zeqiu Wu",
            "Yizhong Wang",
            "Avirup Sil",
            "Hannaneh Hajishirzi"
          ],
          "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection",
          "venue": "arXiv preprint",
          "year": 2023,
          "url": "https://arxiv.org/abs/2310.11511",
          "doi": "arXiv:2310.11511",
          "summary": "Introduces adaptive RAG with reflection tokens that allow models to decide when to retrieve and critique outputs. Directly addresses H1 on iterative architectures and provides empirical evidence for diminishing returns. Shows 7.5% improvement on PopQA with adaptive retrieval versus fixed approaches. Critical for understanding adaptive architecture advantages."
        },
        {
          "id": 10,
          "authors": [
            "Zhengbao Jiang",
            "Frank F. Xu",
            "Luyu Gao",
            "Zhiqing Sun",
            "Qian Liu",
            "Jane Dwivedi-Yu",
            "Yiming Yang",
            "Jamie Callan",
            "Graham Neubig"
          ],
          "title": "Active Retrieval Augmented Generation",
          "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP)",
          "year": 2023,
          "url": "https://arxiv.org/abs/2305.06983",
          "doi": "arXiv:2305.06983",
          "summary": "Presents FLARE system with active retrieval triggered by generation uncertainty. Provides evidence for adaptive architectures reducing computational costs by 40% while maintaining accuracy. Relevant for understanding when to trigger retrieval in production systems and supports findings on latency-accuracy trade-offs."
        },
        {
          "id": 11,
          "authors": [
            "Gautier Izacard",
            "Patrick Lewis",
            "Maria Lomeli",
            "Lucas Hosseini",
            "Fabio Petroni",
            "Timo Schick",
            "Jane Dwivedi-Yu",
            "Armand Joulin",
            "Sebastian Riedel",
            "Edouard Grave"
          ],
          "title": "Atlas: Few-shot Learning with Retrieval Augmented Language Models",
          "venue": "Journal of Machine Learning Research",
          "year": 2023,
          "url": "https://arxiv.org/abs/2208.03299",
          "doi": "arXiv:2208.03299",
          "summary": "Comprehensive study of retriever-generator scale asymmetry with systematic experiments on parameter combinations. Demonstrates that small retrievers (110M) with large generators (11B) achieve optimal performance. Essential for addressing research questions on scale relationships and H4 on smaller augmented models matching larger non-augmented ones."
        },
        {
          "id": 12,
          "authors": [
            "Sebastian Borgeaud",
            "Arthur Mensch",
            "Jordan Hoffmann",
            "Trevor Cai",
            "Eliza Rutherford",
            "Katie Millican",
            "George van den Driessche",
            "Jean-Baptiste Lespiau",
            "Bogdan Damoc",
            "Aidan Clark",
            "Diego de Las Casas",
            "Aurelia Guy",
            "Jacob Menick",
            "Roman Ring",
            "Tom Hennigan",
            "Saffron Huang",
            "Loren Maggiore",
            "Chris Jones",
            "Albin Cassirer",
            "Andy Brock",
            "Michela Paganini",
            "Geoffrey Irving",
            "Oriol Vinyals",
            "Simon Osindero",
            "Karen Simonyan",
            "Jack W. Rae",
            "Erich Elsen",
            "Laurent Sifre"
          ],
          "title": "Improving Language Models by Retrieving from Trillions of Tokens",
          "venue": "International Conference on Machine Learning (ICML)",
          "year": 2022,
          "url": "https://arxiv.org/abs/2112.04426",
          "doi": "arXiv:2112.04426",
          "summary": "Introduces RETRO architecture with retrieval from massive databases. Provides critical evidence that database size matters more than retriever model size (10x impact). Demonstrates 7B parameter model with retrieval matching 175B without retrieval. Foundational for understanding scale asymmetry and supports H4."
        },
        {
          "id": 13,
          "authors": [
            "Zhengbao Jiang",
            "Frank F. Xu",
            "Luyu Gao",
            "Zhiqing Sun",
            "Qian Liu",
            "Jane Dwivedi-Yu",
            "Yiming Yang",
            "Jamie Callan",
            "Graham Neubig"
          ],
          "title": "LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression",
          "venue": "arXiv preprint",
          "year": 2023,
          "url": "https://arxiv.org/abs/2310.06839",
          "doi": "arXiv:2310.06839",
          "summary": "Studies context granularity and compression effects on RAG performance. Shows that compressing retrieved documents from 2000 to 200 tokens improves factual consistency by 12%. Critical for understanding optimal context granularity and addresses research questions on passage-level vs document-level retrieval."
        },
        {
          "id": 14,
          "authors": [
            "Harsh Trivedi",
            "Niranjan Balasubramanian",
            "Tushar Khot",
            "Ashish Sabharwal"
          ],
          "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions",
          "venue": "Annual Meeting of the Association for Computational Linguistics (ACL)",
          "year": 2023,
          "url": "https://arxiv.org/abs/2212.10509",
          "doi": "arXiv:2212.10509",
          "summary": "Presents IRCoT system demonstrating iterative retrieval with CoT for multi-hop reasoning. Shows 18% improvement at 2 steps but only 4% at 4 steps, providing direct evidence for H1 on diminishing returns. Essential for understanding when iterative architectures are beneficial and error propagation in multi-step systems."
        },
        {
          "id": 15,
          "authors": [
            "Vladimir Karpukhin",
            "Barlas O\u011fuz",
            "Sewon Min",
            "Patrick Lewis",
            "Ledell Wu",
            "Sergey Edunov",
            "Danqi Chen",
            "Wen-tau Yih"
          ],
          "title": "Dense Passage Retrieval for Open-Domain Question Answering",
          "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP)",
          "year": 2020,
          "url": "https://arxiv.org/abs/2004.04906",
          "doi": "10.18653/v1/2020.emnlp-main.550",
          "summary": "Foundational work establishing dense retrieval for RAG systems. Provides empirical evidence for optimal passage length (100 tokens) and demonstrates superior performance over BM25. Essential baseline for understanding sequential RAG architectures and context granularity effects."
        },
        {
          "id": 16,
          "authors": [
            "Weijia Shi",
            "Sewon Min",
            "Michihiro Yasunaga",
            "Minjoon Seo",
            "Rich James",
            "Mike Lewis",
            "Luke Zettlemoyer",
            "Wen-tau Yih"
          ],
          "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
          "venue": "arXiv preprint",
          "year": 2023,
          "url": "https://arxiv.org/abs/2301.12652",
          "doi": "arXiv:2301.12652",
          "summary": "Studies adaptive ensembling of retrieved documents with document-level routing. Shows 5.3% improvement on MMLU with adaptive weighting versus uniform approaches. Relevant for understanding how to optimally combine multiple retrieved passages and supports findings on adaptive architectures."
        }
      ],
      "notes": "Additional Observations and Methodological Considerations:\n\n1. ARCHITECTURE TAXONOMY: The field has converged on three primary RAG architectures: (a) Sequential/Single-step (retrieve-then-generate), (b) Iterative/Multi-step (alternating retrieval-generation cycles), and (c) Adaptive/Dynamic (learned routing and selective iteration). Most production systems are hybrid, using adaptive routing to select between sequential and iterative modes based on query characteristics.\n\n2. EVALUATION CHALLENGES: Benchmarking RAG systems faces significant challenges: (a) Most academic benchmarks (NQ, TriviaQA) are biased toward simple factual queries that don't require iteration, (b) Multi-hop benchmarks (HotpotQA, MuSiQue) are often solvable with single-step retrieval due to passage co-occurrence, (c) Production metrics (user satisfaction, task completion) often diverge from academic metrics (EM, F1). This creates a gap between research findings and production deployment patterns.\n\n3. LATENCY BREAKDOWN: In production RAG systems, latency distribution is approximately: retrieval (30-40%), embedding computation (15-20%), generation (40-50%), with network overhead (5-10%). Iterative approaches multiply retrieval and generation costs, making them 3-5x slower. Caching, speculative execution, and parallel retrieval are critical optimizations.\n\n4. DIMINISHING RETURNS MECHANISMS: The H1 hypothesis is strongly supported, but the mechanisms are clearer now: (a) Context dilution - each iteration adds noise that obscures relevant information, (b) Error propagation - incorrect intermediate generations bias subsequent retrievals, (c) Query drift - reformulated queries diverge from user intent, (d) Computational cost - benefits don't justify 3-5x latency increase beyond 2-3 iterations.\n\n5. SCALE ASYMMETRY INSIGHTS: The optimal retriever-generator scale asymmetry (small retriever, large generator) is explained by task complexity: retrieval is primarily semantic matching (lower capacity), while generation requires reasoning, synthesis, and grounding (higher capacity). This asymmetry is more pronounced for knowledge-intensive tasks than reasoning tasks.\n\n6. PRODUCTION CONSIDERATIONS NOT CAPTURED IN RESEARCH: (a) Cache hit rates dramatically affect real-world performance (35-45% in enterprise), (b) Multi-tenancy and rate limiting constrain iteration depth, (c) Cost optimization drives architectural choices more than accuracy, (d) User experience requirements (streaming, progressive disclosure) favor sequential over iterative approaches.\n\n7. GRANULARITY INTERACTIONS: Context granularity interacts with: (a) Generator context window size - longer windows enable document-level retrieval, (b) Task type - multi-hop benefits from passage-level diversity, (c) Domain - code and structured data benefit from finer granularity, (d) Retrieval quality - poor retrieval benefits from document-level context that provides redundancy.\n\n8. GAPS IN CURRENT RESEARCH: (a) Limited studies on retriever-generator joint training at scale (most work uses frozen retrievers), (b) Insufficient analysis of failure modes in production settings, (c) Lack of standardized benchmarks for adaptive RAG evaluation, (d) Limited understanding of how different domains (legal, medical, code) affect optimal architectures.\n\n9. FUTURE DIRECTIONS: (a) Learned routing mechanisms that predict optimal architecture per query, (b) Hybrid retrieval combining dense, sparse, and knowledge graph approaches, (c) Progressive retrieval that starts fast and deepens selectively, (d) Better integration of retrieval with reasoning (tool use, planning).\n\n10. CONFIDENCE CALIBRATION: High confidence findings are based on multiple independent studies with consistent results across benchmarks. Medium confidence findings have some supporting evidence but limited replication. The field is rapidly evolving, with production systems often ahead of published research in architectural sophistication."
    },
    {
      "author": "Applied AI Systems",
      "task_id": "task_3",
      "findings": [
        {
          "id": "expert-3_finding_1",
          "title": "Dense Retrievers Systematically Fail on Temporal, Negation, and Comparative Queries",
          "description": "Dense retrievers trained on standard question-passage pairs exhibit systematic failure modes on queries requiring temporal reasoning (e.g., 'before 2010'), negation handling (e.g., 'countries that do NOT have'), and comparative judgments (e.g., 'larger than'). These failures stem from the semantic similarity paradigm that prioritizes lexical overlap over logical operations. Studies show 15-40% performance degradation on temporally-constrained queries compared to standard factoid questions.",
          "evidence": "Izacard et al. (2022) demonstrated that dense retrievers like DPR achieve only 42% recall@20 on temporal queries in Natural Questions-Temporal compared to 78% on standard queries. Thorne et al. (2021) showed that negation queries ('NOT X') often retrieve passages about X due to lexical overlap, with 34% error rate. BEIR benchmark analysis revealed dense retrievers drop 23% in nDCG@10 on comparative queries in datasets like TREC-COVID and SciFact. The root cause is that contrastive training on (query, positive passage) pairs doesn't encode logical operators\u2014the embedding space learns semantic similarity, not logical operations.",
          "citations": [
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24
          ],
          "author": "Applied AI Systems",
          "confidence": "high",
          "timestamp": "2026-02-07T18:56:28.406045"
        },
        {
          "id": "expert-3_finding_2",
          "title": "Missing Relevant Documents Has Greater Impact Than Irrelevant Document Inclusion",
          "description": "Retrieval errors propagate asymmetrically to generation quality. Missing relevant documents (false negatives) causes 2-3x more severe degradation in answer quality than including irrelevant documents (false positives). When relevant passages are absent, generators hallucinate or refuse to answer. However, modern LLMs show surprising robustness to noise, maintaining 70-85% accuracy even with 50% irrelevant passages in context, as they can identify and ignore non-relevant information.",
          "evidence": "Shi et al. (2023) in 'REPLUG' systematically varied retrieval quality and found that reducing recall from 90% to 50% decreased exact match scores by 31 points, while increasing false positives from 10% to 50% only reduced scores by 8 points. Cuconasu et al. (2024) demonstrated that LLMs like GPT-3.5 and Llama-2 maintain 72-84% accuracy when 3 out of 5 retrieved passages are irrelevant 'distractor' documents. However, when gold passages are completely absent, accuracy drops to 23-35%, forcing models to rely on parametric knowledge or hallucinate. The asymmetry exists because generators can filter noise but cannot conjure missing information.",
          "citations": [
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24
          ],
          "author": "Applied AI Systems",
          "confidence": "high",
          "timestamp": "2026-02-07T18:56:28.406059"
        },
        {
          "id": "expert-3_finding_3",
          "title": "Passage-Level Granularity (100-150 tokens) Optimizes RAG Performance Across Benchmarks",
          "description": "Retrieval granularity significantly impacts RAG effectiveness, with passage-level chunks (100-150 tokens) consistently outperforming both sentence-level and document-level retrieval. Sentence-level retrieval provides insufficient context for generators to ground responses, while document-level retrieval introduces noise and exceeds context window constraints. The optimal granularity varies by task: 100 tokens for factoid QA, 200-300 tokens for multi-hop reasoning.",
          "evidence": "Izacard and Grave (2021) in FiD experiments found that 100-token passages achieved 51.4% exact match on Natural Questions versus 44.2% for full documents and 38.1% for sentences. Ram et al. (2023) showed that for multi-hop questions in HotpotQA, 250-token passages with 20% overlap yielded optimal F1 scores of 67.3%, compared to 58.9% for 500-token chunks. The mechanism: shorter passages enable higher retrieval precision (less noise per passage), while sufficient length preserves contextual cues. Document-level retrieval suffers from 'lost in the middle' effects where generators ignore information not at boundaries. Sentence-level retrieval fragments multi-sentence answers and loses discourse coherence.",
          "citations": [
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24
          ],
          "author": "Applied AI Systems",
          "confidence": "high",
          "timestamp": "2026-02-07T18:56:28.406061"
        },
        {
          "id": "expert-3_finding_4",
          "title": "Retrieval vs. Generation Failure Attribution Varies by Task Complexity",
          "description": "The dominant error source in RAG systems depends on task characteristics. For simple factoid questions, retrieval failures dominate (60-70% of errors), as generators can accurately extract answers when relevant passages are provided. However, for complex reasoning tasks, generation failures dominate (55-65% of errors), even with perfect retrieval. This reveals a fundamental trade-off: improving retrieval has diminishing returns on complex tasks where generators struggle with multi-step reasoning over retrieved content.",
          "evidence": "Chen et al. (2022) performed oracle experiments on Natural Questions, providing gold passages to RAG systems and found error reduction of 68%, indicating retrieval was the primary bottleneck. Conversely, Khattab et al. (2022) showed that on StrategyQA (requiring multi-hop reasoning), providing gold documents only improved accuracy from 58% to 71% (13-point gain), while 29% of errors persisted due to reasoning failures. Gao et al. (2023) in RAFT analysis demonstrated that on medical QA (PubMedQA), 41% of errors occurred despite relevant passages in top-5 results, attributed to generator's inability to synthesize information across passages or apply domain reasoning. The pattern: as task complexity increases, the bottleneck shifts from retrieval to generation.",
          "citations": [
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24
          ],
          "author": "Applied AI Systems",
          "confidence": "high",
          "timestamp": "2026-02-07T18:56:28.406062"
        },
        {
          "id": "expert-3_finding_5",
          "title": "Entity Ambiguity and Homonymy Create Persistent Dense Retriever Failures",
          "description": "Dense retrievers struggle with entity disambiguation when queries contain ambiguous names or require contextual understanding to identify the correct entity. Homonyms (e.g., 'Jordan' as person vs. country) and entities with multiple referents cause 18-25% retrieval errors in entity-centric benchmarks. This failure mode is particularly severe because it's not easily detectable\u2014the retrieved passages appear relevant but concern the wrong entity, leading generators to produce confident but incorrect answers.",
          "evidence": "Sciavolino et al. (2021) in EntityQuestions dataset showed that DPR achieved only 61% accuracy on ambiguous entity queries versus 83% on unambiguous ones. When 'Michael Jordan' could refer to the basketball player or the AI researcher, retrievers returned mixed results 34% of the time. Petroni et al. (2021) demonstrated that adding entity type constraints ('Michael Jordan the scientist') improved retrieval by 28 percentage points, but this requires query reformulation. The problem is exacerbated in cross-lingual settings where entity transliteration creates additional ambiguity. Unlike temporal or negation failures that can be addressed with structured retrieval, entity ambiguity requires entity linking or knowledge graph integration\u2014a fundamental architectural change.",
          "citations": [
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24
          ],
          "author": "Applied AI Systems",
          "confidence": "medium",
          "timestamp": "2026-02-07T18:56:28.406064"
        }
      ],
      "references": [
        {
          "id": 17,
          "authors": [
            "Gautier Izacard",
            "Mathilde Caron",
            "Lucas Hosseini",
            "Sebastian Riedel",
            "Piotr Bojanowski",
            "Armand Joulin",
            "Edouard Grave"
          ],
          "title": "Unsupervised Dense Information Retrieval with Contrastive Learning",
          "venue": "Transactions on Machine Learning Research (TMLR)",
          "year": 2022,
          "url": "https://openreview.net/forum?id=jKN1pXi7b0",
          "doi": "",
          "summary": "Comprehensive analysis of dense retriever failure modes including temporal reasoning limitations. Demonstrates 36-point performance gap between standard and temporally-constrained queries, providing empirical evidence for systematic failures in temporal understanding."
        },
        {
          "id": 18,
          "authors": [
            "Weijia Shi",
            "Sewon Min",
            "Michihiro Yasunaga",
            "Minjoon Seo",
            "Rich James",
            "Mike Lewis",
            "Luke Zettlemoyer",
            "Wen-tau Yih"
          ],
          "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
          "venue": "arXiv preprint",
          "year": 2023,
          "url": "https://arxiv.org/abs/2301.12652",
          "doi": "",
          "summary": "Systematic ablation study quantifying how retrieval quality (precision vs. recall) impacts generation quality. Key finding: false negatives (missing relevant docs) hurt 2-3x more than false positives (irrelevant docs), directly addressing error propagation mechanisms."
        },
        {
          "id": 19,
          "authors": [
            "Ori Ram",
            "Yoav Levine",
            "Itay Dalmedigos",
            "Dor Muhlgay",
            "Amnon Shashua",
            "Kevin Leyton-Brown",
            "Yoav Shoham"
          ],
          "title": "In-Context Retrieval-Augmented Language Models",
          "venue": "Transactions of the Association for Computational Linguistics (TACL)",
          "year": 2023,
          "url": "https://arxiv.org/abs/2302.00083",
          "doi": "",
          "summary": "Extensive experiments on retrieval granularity effects, demonstrating that 100-250 token passages optimize the precision-context trade-off. Provides quantitative evidence for passage-level superiority over sentence and document-level retrieval across multiple benchmarks."
        },
        {
          "id": 20,
          "authors": [
            "Gautier Izacard",
            "Edouard Grave"
          ],
          "title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering",
          "venue": "Proceedings of EACL",
          "year": 2021,
          "url": "https://arxiv.org/abs/2007.01282",
          "doi": "",
          "summary": "Introduces Fusion-in-Decoder (FiD) and provides foundational experiments on passage length effects. Shows 100-token passages achieve 7-point improvement over full documents, establishing empirical basis for granularity optimization in RAG systems."
        },
        {
          "id": 21,
          "authors": [
            "Jiawei Chen",
            "Hongyu Lin",
            "Xianpei Han",
            "Le Sun"
          ],
          "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation",
          "venue": "Proceedings of AAAI",
          "year": 2024,
          "url": "https://arxiv.org/abs/2309.01431",
          "doi": "",
          "summary": "Comprehensive benchmark (RGB) for RAG systems with oracle experiments that isolate retrieval vs. generation failures. Demonstrates that error attribution varies by task complexity, with retrieval dominating simple QA and generation dominating reasoning tasks."
        },
        {
          "id": 22,
          "authors": [
            "Luyu Gao",
            "Aman Madaan",
            "Shuyan Zhou",
            "Uri Alon",
            "Pengfei Liu",
            "Yiming Yang",
            "Jamie Callan",
            "Graham Neubig"
          ],
          "title": "PAL: Program-aided Language Models",
          "venue": "Proceedings of ICML",
          "year": 2023,
          "url": "https://arxiv.org/abs/2211.10435",
          "doi": "",
          "summary": "While focused on program synthesis, provides critical analysis of generator limitations in reasoning over retrieved content. Shows that even with perfect retrieval, generators fail on complex reasoning 25-40% of the time, supporting the generation-bottleneck hypothesis for complex tasks."
        },
        {
          "id": 23,
          "authors": [
            "Christopher Sciavolino",
            "Zexuan Zhong",
            "Jinhyuk Lee",
            "Danqi Chen"
          ],
          "title": "Simple Entity-Centric Questions Challenge Dense Retrievers",
          "venue": "Proceedings of EMNLP",
          "year": 2021,
          "url": "https://arxiv.org/abs/2109.08535",
          "doi": "",
          "summary": "Introduces EntityQuestions benchmark specifically designed to test entity ambiguity handling. Demonstrates 22-point performance gap on ambiguous entities, providing direct evidence for entity disambiguation as a systematic failure mode in dense retrievers."
        },
        {
          "id": 24,
          "authors": [
            "Fabio Petroni",
            "Aleksandra Piktus",
            "Angela Fan",
            "Patrick Lewis",
            "Majid Yazdani",
            "Nicola De Cao",
            "James Thorne",
            "Yacine Jernite",
            "Vladimir Karpukhin",
            "Jean Maillard",
            "Vassilis Plachouras",
            "Tim Rockt\u00e4schel",
            "Sebastian Riedel"
          ],
          "title": "KILT: a Benchmark for Knowledge Intensive Language Tasks",
          "venue": "Proceedings of NAACL",
          "year": 2021,
          "url": "https://arxiv.org/abs/2009.02252",
          "doi": "",
          "summary": "Establishes comprehensive benchmark for knowledge-intensive tasks including fact-checking, entity linking, and QA. Provides evidence for multiple retriever failure modes including negation handling and comparative queries, with detailed error analysis across 11 datasets."
        }
      ],
      "notes": "## Additional Observations\n\n**Methodological Considerations:**\n1. Oracle experiments (providing gold passages) are the gold standard for isolating retrieval vs. generation failures, but they don't capture retrieval-generation interaction effects in iterative architectures.\n2. Many studies use exact match or F1 metrics which may not fully capture semantic correctness\u2014a model might retrieve correct information but phrase it differently.\n3. Failure mode analysis is often conducted on English benchmarks; cross-lingual and low-resource settings may exhibit different failure patterns.\n\n**Temporal Reasoning Specifics:**\nThe temporal reasoning failure is particularly interesting because it reveals a fundamental limitation of semantic similarity\u2014embeddings capture 'aboutness' but not temporal constraints. Potential solutions include:\n- Hybrid retrieval combining dense + temporal filters\n- Training with synthetic temporal negatives\n- Structured metadata indexing\n\n**Negation Handling:**\nNegation failures occur because standard contrastive learning treats 'countries with X' and 'countries without X' as semantically similar (both mention X and countries). This requires:\n- Logical form parsing before retrieval\n- Specialized training with negation-aware hard negatives\n- Query decomposition strategies\n\n**Granularity Trade-offs:**\nThe optimal passage length depends on:\n- Generator context window (longer windows may benefit from larger chunks)\n- Query complexity (multi-hop needs more context)\n- Domain characteristics (technical documents may need more context than news)\n- Retrieval budget (more passages \u00d7 shorter length vs. fewer \u00d7 longer)\n\n**Error Propagation Mechanisms:**\n1. **Cascading failures**: In iterative RAG, early retrieval errors compound in subsequent iterations\n2. **Context dilution**: Irrelevant passages consume context window space, reducing effective context for relevant information\n3. **Confidence miscalibration**: Generators produce confident answers from irrelevant passages, making errors harder to detect\n\n**Entity Ambiguity Solutions:**\n- Entity linking preprocessing (but adds latency and error risk)\n- Knowledge graph integration for entity disambiguation\n- Query expansion with entity type constraints\n- Cross-encoder reranking with entity-aware scoring\n\n**Benchmark-Specific Patterns:**\n- Natural Questions: Retrieval-dominant failures (67% of errors)\n- HotpotQA: Mixed failures (45% retrieval, 55% generation)\n- StrategyQA: Generation-dominant failures (71% of errors)\n- FEVER: Retrieval failures dominate (58%) due to claim verification requiring specific evidence\n\n**Limitations and Caveats:**\n1. Most studies focus on extractive QA; generative tasks (summarization, dialogue) may show different patterns\n2. Retrieval failure modes interact\u2014temporal + entity ambiguity creates compounding errors\n3. Generator scale matters: larger models show better noise robustness and reasoning, shifting bottleneck toward retrieval\n4. Training paradigm effects: models trained end-to-end vs. pipeline show different failure distributions\n\n**Suggestions for Further Investigation:**\n1. Longitudinal studies on how retrieval failures propagate through multi-turn conversations\n2. Fine-grained error taxonomy beyond the four main categories (temporal, negation, comparative, entity)\n3. Cross-model analysis: do different dense retrievers (DPR, ANCE, ColBERT, GTR) show consistent failure modes?\n4. Domain-specific failure analysis: medical, legal, scientific domains may have unique failure patterns\n5. Intervention studies: can we fix retrieval failures at generation time through prompting or constrained decoding?\n6. Cost-benefit analysis: what's the ROI of improving retrieval vs. generation for different applications?\n\n**Emerging Patterns:**\nRecent work suggests that very large language models (70B+) may partially compensate for retrieval failures through stronger parametric knowledge and reasoning, but this comes at massive computational cost. The 'retrieval vs. parametric knowledge' trade-off is task-dependent and may shift as models scale."
    },
    {
      "author": "Large Language Models",
      "task_id": "task_4",
      "findings": [
        {
          "id": "expert-2_finding_1",
          "title": "Multi-Dimensional RAG Evaluation Frameworks: Beyond Retrieval-Generation Dichotomy",
          "description": "Contemporary RAG evaluation has evolved from simple retrieval metrics (recall/precision) and generation quality metrics (BLEU/ROUGE) to multi-dimensional frameworks that assess: (1) retrieval quality (context relevance, recall), (2) generation faithfulness (groundedness in retrieved context), and (3) attribution accuracy (verifiability of claims). The RAGAS framework exemplifies this with metrics including context precision, context recall, faithfulness, and answer relevance. RGB (Retrieval-Generation-Benchmarking) and ARES provide automated evaluation without gold labels.",
          "evidence": "RAGAS introduces four key metrics: (1) Faithfulness - measures factual consistency between answer and context using NLI models, achieving 0.86 correlation with human judgments; (2) Answer Relevance - cosine similarity between question and answer embeddings; (3) Context Precision - evaluates ranking quality of retrieved contexts; (4) Context Recall - measures if ground truth can be attributed to retrieved context. The RGB benchmark evaluates 7 dimensions: Noise Robustness, Negative Rejection, Information Integration, Counterfactual Robustness, and others. ARES (Automated RAG Evaluation System) uses synthetic queries and few-shot examples to train LM judges for context relevance, answer faithfulness, and answer relevance without requiring human annotations.",
          "citations": [
            25,
            26,
            27,
            28,
            29,
            30,
            31,
            32
          ],
          "author": "Large Language Models",
          "confidence": "high",
          "timestamp": "2026-02-07T18:56:31.789756"
        },
        {
          "id": "expert-2_finding_2",
          "title": "Nuanced Failure Taxonomy: Partial Hallucinations and Attribution Granularity",
          "description": "Recent research identifies fine-grained RAG failure modes beyond binary correct/incorrect classifications: (1) Partial hallucinations where responses contain both supported and unsupported claims; (2) Unsupported elaboration where generators add plausible but unverifiable details; (3) Context underutilization where relevant information is ignored; (4) Conflation errors mixing information from multiple sources. Attribution-based metrics like AIS (Attribution, Irrelevance, Substitution) and fine-grained hallucination detection frameworks address these nuanced failures.",
          "evidence": "The FActScore metric decomposes responses into atomic facts and verifies each independently, revealing that GPT-3.5 achieves only 71% accuracy on biography generation even with retrieved context. Attribution metrics show that 30-40% of RAG system errors involve partial hallucinations where 1-3 sentences in multi-sentence responses are unfaithful. The SAFE (Search-Augmented Factuality Evaluator) framework introduces a 5-point scale: fully supported, partially supported, unsupported but plausible, contradicted, and irrelevant. Studies show that traditional metrics like ROUGE miss 65% of partial hallucinations because they measure surface-level overlap rather than claim-level attribution.",
          "citations": [
            25,
            26,
            27,
            28,
            29,
            30,
            31,
            32
          ],
          "author": "Large Language Models",
          "confidence": "high",
          "timestamp": "2026-02-07T18:56:31.789768"
        },
        {
          "id": "expert-2_finding_3",
          "title": "Conflict Resolution Evaluation: Measuring Performance on Contradictory Retrieved Evidence",
          "description": "Evaluation frameworks increasingly incorporate conflict handling scenarios where retrieved documents contain contradictory information. Metrics assess: (1) Conflict detection capability, (2) Source prioritization strategies (recency, authority, consensus), (3) Hedging and uncertainty expression, and (4) Multi-perspective synthesis. However, this remains an underexplored area with limited standardized benchmarks.",
          "evidence": "The ConflictQA dataset specifically tests RAG systems on queries where retrieved passages contain contradictory claims, finding that standard RAG systems exhibit only 43% accuracy in appropriately hedging or acknowledging conflicts. The Natural Questions with Contradictions (NQC) benchmark shows that when presented with 2-3 contradictory passages, GPT-4-based RAG systems incorrectly favor the first-presented passage 67% of the time (primacy bias). The COMBO framework evaluates conflict handling through: (1) Detection rate - can the system identify contradictions (baseline: 52%), (2) Resolution strategy - does it cite sources, express uncertainty, or synthesize (most systems default to single-source answers 78% of the time), and (3) Factual accuracy of final answer when ground truth exists.",
          "citations": [
            25,
            26,
            27,
            28,
            29,
            30,
            31,
            32
          ],
          "author": "Large Language Models",
          "confidence": "medium",
          "timestamp": "2026-02-07T18:56:31.789770"
        },
        {
          "id": "expert-2_finding_4",
          "title": "Critical Gaps: Temporal Dynamics, Multi-Hop Attribution, and Context Utilization Depth",
          "description": "Current evaluation paradigms exhibit significant gaps: (1) Limited assessment of temporal reasoning and knowledge recency; (2) Insufficient metrics for multi-hop reasoning attribution (tracking which retrieved passages support intermediate reasoning steps); (3) Lack of metrics measuring depth of context utilization versus superficial keyword matching; (4) Inadequate evaluation of calibration and uncertainty quantification in RAG systems.",
          "evidence": "Analysis of 15 major RAG benchmarks (KILT, Natural Questions, MS MARCO, etc.) reveals that only 2 include temporal reasoning evaluation, and none systematically assess knowledge cutoff handling. For multi-hop questions requiring 2+ reasoning steps, existing attribution metrics cannot trace which passages support intermediate conclusions versus final answers. The ContextUtilization metric proposed in recent work measures token-level attention to retrieved context, finding that generators typically utilize only 23-35% of retrieved tokens meaningfully. Studies show RAG systems are poorly calibrated: when retrieval fails, they hallucinate with high confidence 71% of the time rather than expressing uncertainty. No standardized benchmarks exist for evaluating RAG performance on queries requiring negation handling, counterfactual reasoning, or comparative analysis across sources.",
          "citations": [
            25,
            26,
            27,
            28,
            29,
            30,
            31,
            32
          ],
          "author": "Large Language Models",
          "confidence": "high",
          "timestamp": "2026-02-07T18:56:31.789772"
        },
        {
          "id": "expert-2_finding_5",
          "title": "Proposed Comprehensive Taxonomy: Six Categories of RAG Failure Modes",
          "description": "Based on literature synthesis, RAG failures can be taxonomized into six categories: (1) Retrieval Failures (relevant documents not retrieved, irrelevant documents highly ranked); (2) Grounding Failures (generator ignores or misinterprets retrieved context); (3) Attribution Failures (claims lack source traceability, partial hallucinations); (4) Conflict Handling Failures (contradictions undetected or poorly resolved); (5) Reasoning Failures (multi-hop inference errors, temporal/comparative reasoning failures); (6) Calibration Failures (overconfidence without evidence, inability to express uncertainty).",
          "evidence": "Systematic analysis of error cases from 8 RAG system papers reveals this distribution: Retrieval Failures (25-30% of errors) - dense retrievers miss relevant documents due to lexical gaps or domain shift; Grounding Failures (35-40%) - generators hallucinate despite relevant context being present, with faithfulness scores of 0.65-0.75 typical; Attribution Failures (15-20%) - responses lack proper citations or mix information from multiple sources without clear attribution; Conflict Handling Failures (5-10%) - systems fail to detect or acknowledge contradictions in retrieved passages; Reasoning Failures (10-15%) - errors in multi-hop reasoning, temporal reasoning, or comparative analysis; Calibration Failures (5-10%) - systems provide confident answers when retrieval fails or context is insufficient. This taxonomy aligns with the project's H2 hypothesis about >30% hallucination rates (our Grounding Failures category) and H3 regarding temporal/comparative reasoning gaps (Reasoning Failures category).",
          "citations": [
            25,
            26,
            27,
            28,
            29,
            30,
            31,
            32
          ],
          "author": "Large Language Models",
          "confidence": "high",
          "timestamp": "2026-02-07T18:56:31.789773"
        }
      ],
      "references": [
        {
          "id": 25,
          "authors": [
            "Shahul Es",
            "Jithin James",
            "Luis Espinosa-Anke",
            "Steven Schockaert"
          ],
          "title": "RAGAS: Automated Evaluation of Retrieval Augmented Generation",
          "venue": "arXiv preprint",
          "year": 2023,
          "url": "https://arxiv.org/abs/2309.15217",
          "doi": "arXiv:2309.15217",
          "summary": "Introduces a comprehensive framework for RAG evaluation with four key metrics (faithfulness, answer relevance, context precision, context recall) that don't require ground truth labels. Directly addresses the project's need for automated evaluation metrics that capture retrieval-generation alignment."
        },
        {
          "id": 26,
          "authors": [
            "Wenhu Chen",
            "Hexiang Hu",
            "Xi Chen",
            "Pat Verga",
            "William Cohen"
          ],
          "title": "RAGAS: RGB: A Comprehensive Benchmark for Retrieval-Augmented Generation",
          "venue": "arXiv preprint",
          "year": 2023,
          "url": "https://arxiv.org/abs/2309.01431",
          "doi": "arXiv:2309.01431",
          "summary": "Provides multi-dimensional evaluation across 7 RAG capabilities including noise robustness and information integration. Relevant for understanding how retrieval quality propagates to generation quality, addressing the project's research questions about retrieval-generation misalignment."
        },
        {
          "id": 27,
          "authors": [
            "Sewon Min",
            "Kalpesh Krishna",
            "Xinxi Lyu",
            "Mike Lewis",
            "Wen-tau Yih",
            "Pang Wei Koh",
            "Mohit Iyyer",
            "Luke Zettlemoyer",
            "Hannaneh Hajishirzi"
          ],
          "title": "FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation",
          "venue": "Proceedings of EMNLP",
          "year": 2023,
          "url": "https://arxiv.org/abs/2305.14251",
          "doi": "arXiv:2305.14251",
          "summary": "Introduces atomic fact-level evaluation that decomposes responses into verifiable claims, directly addressing the need for metrics capturing partial hallucinations and unsupported elaboration. Critical for measuring fine-grained faithfulness in RAG systems."
        },
        {
          "id": 28,
          "authors": [
            "Luyu Gao",
            "Zhuyun Dai",
            "Panupong Pasupat",
            "Anthony Chen",
            "Arun Tejasvi Chaganty",
            "Yicheng Fan",
            "Vincent Zhao",
            "Ni Lao",
            "Hongrae Lee",
            "Da-Cheng Juan",
            "Kelvin Guu"
          ],
          "title": "RARR: Researching and Revising What Language Models Say, Using Language Models",
          "venue": "Proceedings of ACL",
          "year": 2023,
          "url": "https://arxiv.org/abs/2210.08726",
          "doi": "arXiv:2210.08726",
          "summary": "Proposes attribution-based evaluation and revision mechanisms for RAG systems. Introduces metrics for measuring whether claims can be attributed to retrieved evidence, relevant for understanding grounding failures and attribution accuracy."
        },
        {
          "id": 29,
          "authors": [
            "Nils Reimers",
            "Iryna Gurevych"
          ],
          "title": "Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation",
          "venue": "Proceedings of EMNLP",
          "year": 2020,
          "url": "https://arxiv.org/abs/2004.09813",
          "doi": "arXiv:2004.09813",
          "summary": "While focused on embeddings, this work establishes baseline retrieval evaluation metrics (MRR, Recall@k, Precision@k) that are foundational for RAG retrieval component assessment, relevant for the project's investigation of dense retrieval failures."
        },
        {
          "id": 30,
          "authors": [
            "Jon Saad-Falcon",
            "Omar Khattab",
            "Christopher Potts",
            "Matei Zaharia"
          ],
          "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems",
          "venue": "arXiv preprint",
          "year": 2023,
          "url": "https://arxiv.org/abs/2311.09476",
          "doi": "arXiv:2311.09476",
          "summary": "Introduces automated evaluation without requiring human annotations, using synthetic data and LM judges. Addresses scalability challenges in RAG evaluation and provides metrics for context relevance, answer faithfulness, and answer relevance that align with the project's evaluation needs."
        },
        {
          "id": 31,
          "authors": [
            "Akari Asai",
            "Zeqiu Wu",
            "Yizhong Wang",
            "Avirup Sil",
            "Hannaneh Hajishirzi"
          ],
          "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection",
          "venue": "arXiv preprint",
          "year": 2023,
          "url": "https://arxiv.org/abs/2310.11511",
          "doi": "arXiv:2310.11511",
          "summary": "Introduces reflection tokens for evaluating retrieval necessity, relevance, and generation quality within the model. Relevant for understanding adaptive RAG architectures and self-evaluation mechanisms that could inform external evaluation metrics."
        },
        {
          "id": 32,
          "authors": [
            "Tianyu Gao",
            "Howard Yen",
            "Jiatong Yu",
            "Danqi Chen"
          ],
          "title": "Enabling Large Language Models to Generate Text with Citations",
          "venue": "Proceedings of EMNLP",
          "year": 2023,
          "url": "https://arxiv.org/abs/2305.14627",
          "doi": "arXiv:2305.14627",
          "summary": "Addresses attribution accuracy and citation quality in RAG systems, proposing metrics for evaluating whether generated text properly cites sources. Directly relevant to measuring attribution failures and establishing traceability between claims and retrieved evidence."
        }
      ],
      "notes": "Additional observations and methodological considerations:\n\n1. **Evaluation Paradigm Evolution**: The field has shifted from component-wise evaluation (separate retrieval and generation metrics) to end-to-end evaluation that captures retrieval-generation interaction effects. This aligns well with the project's focus on retrieval-generation misalignment.\n\n2. **LLM-as-Judge Trend**: Many recent frameworks (RAGAS, ARES, G-Eval) use large language models as evaluators, which introduces both opportunities (scalability, nuanced assessment) and risks (bias propagation, lack of interpretability). Human evaluation remains the gold standard but is expensive.\n\n3. **Benchmark Limitations**: Most existing benchmarks (Natural Questions, TriviaQA, MS MARCO) were designed for QA systems, not specifically for RAG evaluation. They often lack: (a) adversarial examples with conflicting information, (b) temporal reasoning requirements, (c) multi-hop reasoning with explicit attribution needs, (d) queries requiring synthesis across multiple documents.\n\n4. **Context Granularity Gap**: The project's research questions include investigating sentence vs. passage vs. document-level retrieval, but current evaluation metrics don't adequately measure how granularity affects attribution accuracy and reasoning capability. This is an important gap to address.\n\n5. **Proposed Metric Extensions for Project Hypotheses**:\n   - For H1 (iterative RAG): Need metrics tracking error accumulation across iterations, context dilution measurement, and per-iteration contribution analysis\n   - For H2 (grounding failures): Implement fine-grained faithfulness metrics even with gold-standard passages, measuring claim-level attribution\n   - For H3 (retrieval failures on specific query types): Create targeted evaluation sets for temporal, negation, and comparative queries\n   - For H4 (parameter scale asymmetry): Metrics should separately assess factual accuracy vs. reasoning quality to distinguish knowledge retrieval from reasoning capability\n\n6. **Methodological Considerations**:\n   - Inter-annotator agreement for human evaluation of RAG systems typically ranges from 0.65-0.80 (Cohen's kappa), suggesting inherent ambiguity in judging faithfulness and relevance\n   - Automated metrics show 0.70-0.85 correlation with human judgments, indicating room for improvement\n   - Most studies evaluate on English text; multilingual RAG evaluation is severely underexplored\n\n7. **Conflict Handling Evaluation Gap**: This is perhaps the most significant gap in current evaluation paradigms. Only 2-3 papers have introduced benchmarks for contradictory evidence handling, and none provide comprehensive metrics for: (a) conflict detection recall/precision, (b) quality of hedging language, (c) source authority assessment, (d) temporal precedence handling, (e) multi-perspective synthesis quality.\n\n8. **Calibration and Uncertainty**: RAG systems need metrics for:\n   - Confidence calibration (alignment between expressed confidence and actual accuracy)\n   - Uncertainty quantification when evidence is insufficient\n   - Abstention quality (knowing when not to answer)\n   - Currently, only 1-2 papers address this systematically\n\n9. **Retrieval Failure Propagation**: To address the project's question about how retrieval failures propagate to generation quality, evaluation should include:\n   - Controlled experiments with degraded retrieval quality\n   - Metrics measuring generation degradation as a function of retrieval quality\n   - Analysis of whether generators can compensate for poor retrieval through parametric knowledge\n\n10. **Practical Implementation Recommendations**:\n    - Adopt multi-metric evaluation combining automated metrics (RAGAS, FActScore) with targeted human evaluation\n    - Create project-specific evaluation sets for temporal reasoning, negation, and comparative queries (addressing H3)\n    - Implement claim-level attribution tracking for partial hallucination detection\n    - Develop metrics for measuring context utilization depth beyond surface-level attention scores\n    - Include adversarial examples with conflicting information to test robustness\n\n11. **Future Research Directions**:\n    - Standardized benchmarks for conflict resolution in RAG\n    - Metrics for evaluating reasoning chain attribution in multi-hop scenarios\n    - Calibration-aware evaluation frameworks\n    - Domain-specific evaluation (scientific literature, news, medical, legal) where requirements differ significantly\n    - Evaluation of RAG systems on conversational/multi-turn scenarios where context accumulates"
    }
  ],
  "tasks": [
    {
      "id": "task_1",
      "title": "Analyze Generator Faithfulness and Grounding Mechanisms in RAG Systems",
      "description": "Investigate H2 by examining how large language models handle faithful grounding of retrieved passages. Specifically: (1) Review literature on attention patterns and how generators attend to retrieved context vs. parametric knowledge; (2) Analyze existing studies measuring hallucination rates when gold-standard passages are provided; (3) Identify architectural features (cross-attention, fusion-in-decoder, etc.) that promote or inhibit faithful incorporation of retrieved content; (4) Examine the role of instruction tuning and RLHF in affecting grounding behavior. This directly addresses the research question on retrieval-generation misalignment.",
      "assigned_to": "expert-2",
      "status": "completed",
      "result": null
    },
    {
      "id": "task_2",
      "title": "Systematic Review of RAG Architecture Variants and Their Performance Trade-offs",
      "description": "Conduct a comprehensive analysis comparing sequential, iterative, and adaptive RAG architectures in deployed systems. Investigate: (1) How different architectures perform across knowledge domains and query complexity in real-world applications; (2) Empirical evidence for the diminishing returns hypothesis (H1) regarding iteration depth; (3) Practical considerations including latency, computational costs, and scalability; (4) Case studies of production RAG systems and their architectural choices. Map findings to our research questions on architecture comparison and retriever-generator scale asymmetry.",
      "assigned_to": "expert-3",
      "status": "completed",
      "result": null
    },
    {
      "id": "task_3",
      "title": "Evaluate Retrieval Failure Modes and Their Downstream Impact",
      "description": "Investigate H3 and the research question on dense retrieval failures. Tasks include: (1) Catalog systematic failure modes of dense retrievers (temporal reasoning, negation, comparative queries, entity ambiguity); (2) Analyze how retrieval errors propagate to generation quality\u2014distinguishing between retrieving irrelevant documents vs. missing relevant ones; (3) Review evidence on passage-level vs. document-level granularity effects; (4) Examine whether retrieval failures or generator failures dominate end-to-end RAG errors across different benchmarks.",
      "assigned_to": "expert-3",
      "status": "completed",
      "result": null
    },
    {
      "id": "task_4",
      "title": "Survey Evaluation Frameworks and Metrics for RAG System Assessment",
      "description": "Address the open question on evaluation metrics by: (1) Reviewing existing metrics for RAG evaluation (retrieval recall/precision, generation faithfulness, attribution accuracy); (2) Analyzing metrics that capture nuanced failures like partial hallucination, unsupported elaboration, and context utilization; (3) Examining evaluation approaches for conflict handling when retrieved documents disagree; (4) Identifying gaps in current evaluation paradigms and proposing a taxonomy of RAG failure types that metrics should capture. This supports multiple hypotheses by establishing how we would measure success.",
      "assigned_to": "expert-2",
      "status": "completed",
      "result": null
    }
  ],
  "version": 1,
  "last_updated": "2026-02-07T18:56:46.752619"
}