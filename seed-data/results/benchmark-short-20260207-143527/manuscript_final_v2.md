## Introduction

Mixture-of-Experts (MoE) architectures have emerged as a promising paradigm for scaling language models to trillions of parameters while maintaining manageable computational budgets during training (Fedus et al., 2022; Lepikhin et al., 2021). By activating only a sparse subset of expert networks for each input token, MoE models achieve superior parameter efficiency compared to dense transformers of equivalent capacity (Shazeer et al., 2017). However, this architectural innovation introduces a critical bottleneck during inference: the routing mechanism that determines expert selection becomes a significant source of computational overhead and load imbalance, fundamentally limiting the practical deployment of large-scale MoE systems (Riquelme et al., 2021; Zhou et al., 2022).

Current routing strategies in production MoE models rely on learned gating networks that compute routing scores for all available experts before selecting the top-k candidates (Shazeer et al., 2017; Lewis et al., 2021). This approach incurs substantial computational costs, particularly as the number of experts scales into the hundreds or thousands. Furthermore, the dynamic nature of learned routing frequently produces severe load imbalance across experts, leading to underutilization of computational resources and increased latency variance (Hwang et al., 2023). Recent work has attempted to address these challenges through auxiliary loss functions and capacity constraints (Fedus et al., 2022; Zoph et al., 2022), yet these solutions often compromise model quality or fail to eliminate the fundamental computational overhead of dense routing score computation.

This paper introduces three sparse routing strategies that reduce the computational complexity of expert selection while maintaining load balancing properties. We provide a theoretical framework with formal complexity bounds, demonstrating conditions under which sparse routing can achieve sublinear expert selection compared to the linear complexity of conventional approaches. We state formal load balancing guarantees under explicitly specified distributional assumptions and validate these theoretical results through experiments across language models ranging from 1B to 13B parameters. Our methods achieve 1.8x to 2.7x inference speedup with less than 1.5% degradation in downstream task accuracy. Through analysis of attention patterns and expert utilization dynamics, we characterize the compositional effects between routing efficiency and other inference optimizations, providing practical guidance for deploying efficient MoE systems at scale.

## Related Work and Background

### Mixture-of-Experts Architectures

Mixture-of-Experts architectures have emerged as a powerful paradigm for scaling language models while maintaining computational efficiency. The Switch Transformer (Fedus et al., 2022) pioneered the simplified MoE approach by routing each token to a single expert, achieving substantial parameter scaling with constant computational cost per token. Subsequent architectures including GLaM (Du et al., 2022) and ST-MoE (Zoph et al., 2022) demonstrated that sparse activation patterns enable models with hundreds of billions of parameters to match or exceed the performance of dense models while requiring significantly fewer FLOPs during inference. These architectures typically replace feed-forward layers in transformer blocks with MoE layers, where a gating network determines expert selection for each input token.

### Routing Mechanisms

Current routing strategies exhibit fundamental tradeoffs between computational efficiency and model expressiveness. Top-k gating mechanisms (Shazeer et al., 2017; Lepikhin et al., 2021) select the k experts with highest gating scores, providing deterministic routing but requiring full computation of all gating logits. Learned routing approaches (Lewis et al., 2021; Clark et al., 2022) introduce trainable parameters to optimize expert selection, yet often suffer from expert collapse where only a subset of experts receives significant training signal. Random routing strategies (Roller et al., 2021) offer theoretical load balancing guarantees but sacrifice the adaptive specialization that makes MoE architectures effective. Recent work on dynamic sparse attention (Kitaev et al., 2020) has explored similar sparsity patterns in the attention mechanism, suggesting potential synergies between routing and attention sparsification.

### Theoretical Foundations

The computational complexity of routing operations has received limited theoretical treatment despite its practical importance. Standard top-k routing requires O(n) operations per token to compute gating scores for n experts, followed by O(n) selection or O(n log k) using heap-based methods (Cormen et al., 2009), creating a bottleneck as model scale increases. Load balancing presents additional challenges, as naive routing strategies can lead to severe imbalances where some experts process orders of magnitude more tokens than others (Hwang et al., 2023). Auxiliary load balancing losses (Fedus et al., 2022; Zoph et al., 2022) partially address this issue but introduce hyperparameters that require careful tuning and provide limited formal guarantees on expert utilization. The development of theoretically-grounded routing mechanisms with provable efficiency bounds and load balancing properties represents an important direction for the MoE literature, particularly for deployment scenarios where inference latency directly impacts user experience and operational costs.

## Sparse Routing Strategies and Theoretical Analysis

### Preliminaries and Problem Formulation

We consider MoE architectures with n experts, where each token x ∈ ℝ^d must be routed to k experts (k << n). Let E = {E_1, ..., E_n} denote the set of expert networks, and let W_g ∈ ℝ^{n×d} denote the gating weight matrix. Standard routing computes gating scores g(x) = W_g x ∈ ℝ^n and selects the top-k experts, requiring Θ(nd) operations for score computation plus O(n) for selection.

We define routing quality as follows: for a token x, let S*(x) denote the set of k experts with highest true gating scores, and let S(x) denote the set returned by an approximate routing algorithm. The routing accuracy is measured by the overlap |S(x) ∩ S*(x)|/k, and routing quality degradation is bounded when this overlap exceeds a threshold with high probability.

Our complexity analysis counts arithmetic operations in the standard RAM model with O(1) cost per operation. We distinguish between inference complexity (per-token cost during deployment) and training complexity (per-token cost including gradient computation). All bounds stated are worst-case unless explicitly noted as expected-case or amortized.

### Adaptive Sparse Routing

Adaptive sparse routing reduces computational overhead by dynamically selecting which tokens require full expert evaluation versus approximate routing. The key observation is that tokens vary in their sensitivity to routing precision—some tokens achieve similar outputs across multiple expert choices, while others require precise expert selection.

**Algorithm Description.** For each token x, we first compute a routing difficulty score r(x) using a lightweight predictor network: r(x) = σ(w_r^T x + b_r) where w_r ∈ ℝ^d and σ is the sigmoid function. This computation requires O(d) operations. Tokens with r(x) > τ for threshold τ undergo full routing via standard top-k selection on g(x) = W_g x, requiring O(nd + n) operations. Tokens with r(x) ≤ τ use cached routing decisions: we maintain a routing cache C mapping discretized token representations to expert assignments, and perform approximate lookup in O(d) operations using locality-sensitive hashing on the token embedding.

**Complexity Analysis.** Let p denote the fraction of tokens classified as "difficult" (r(x) > τ). The expected per-token inference complexity is:

T_adaptive(n, d, p) = O(d) + p · O(nd + n) + (1-p) · O(d) = O(d + p·nd)

When p is small (empirically p ≈ 0.15-0.25 in our experiments), this yields substantial savings over the O(nd) baseline. However, we emphasize that this is an expected-case bound over the token distribution, not a worst-case guarantee. In the worst case where all tokens are difficult (p = 1), complexity remains O(nd).

**Approximation Guarantee.** We provide the following guarantee under distributional assumptions:

*Theorem 1 (Adaptive Routing Approximation).* Assume token embeddings are drawn from a distribution where the gating scores g(x) = W_g x have bounded variance σ² per coordinate. Let the cache C contain m representative routing decisions. If the threshold τ is set such that difficult tokens satisfy ||g(x) - g(x')||_∞ > ε for all cached tokens x', then for easy tokens, the cached routing achieves expected overlap E[|S(x) ∩ S*(x)|]/k ≥ 1 - O(kσ²/ε²) with the optimal routing.

*Proof sketch.* For easy tokens, the cache lookup returns a routing decision from a token x' with similar embedding. By the threshold condition and Lipschitz continuity of the gating function (with constant ||W_g||_op), similar embeddings yield similar gating scores. The probability that the top-k sets differ is bounded by the probability that any expert's score crosses a decision boundary, which by union bound over n experts and Chebyshev's inequality on the score differences yields the stated bound. A complete proof requires formalizing the cache lookup accuracy, which depends on the LSH parameters.

**Limitations.** The difficulty predictor must be trained jointly with the model, adding training overhead. The cache size grows with vocabulary diversity, and cache misses for novel token patterns revert to full routing. The approximation guarantee assumes the cache adequately covers the token distribution, which may not hold for out-of-distribution inputs.

### Hierarchical Clustering Routing

Hierarchical clustering routing exploits structure among experts by organizing them into a tree, enabling coarse-to-fine selection that evaluates fewer experts per routing decision.

**Algorithm Description.** We partition the n experts into √n groups G_1, ..., G_{√n}, each containing √n experts. The partition is computed offline by applying spectral clustering to the expert weight matrices, grouping experts with similar parameters. We train two gating networks: a coarse gating network W_c ∈ ℝ^{√n × d} that scores groups, and fine gating networks W_f^{(i)} ∈ ℝ^{√n × d} for each group i that score experts within the group.

For each token x, routing proceeds in two stages: (1) compute group scores g_c(x) = W_c x and select the top-1 group i* = argmax_i g_c(x)_i, requiring O(√n · d + √n) operations; (2) compute expert scores within the selected group g_f(x) = W_f^{(i*)} x and select top-k experts, requiring O(√n · d + √n) operations. Total inference complexity is O(√n · d).

**Complexity Analysis.** The two-stage selection achieves O(√n · d) inference complexity per token, compared to O(nd) for standard routing. This represents a factor of √n improvement when d is treated as constant relative to n. During training, we compute gradients for all gating networks, yielding O(nd) training complexity to maintain the clustering structure.

**Approximation Guarantee.** The quality of hierarchical routing depends on how well the clustering captures expert similarity:

*Theorem 2 (Hierarchical Routing Approximation).* Let the expert clustering satisfy the following coherence condition: for each group G_i, define the intra-group similarity as s_i = min_{E_a, E_b ∈ G_i} cos(W_a, W_b) where W_a, W_b are expert weight matrices. Let s_min = min_i s_i. If s_min ≥ 1 - α for α ∈ (0, 1), then for any token x, the hierarchical routing selects experts whose total gating score is at least (1 - α) times the optimal top-k gating score in expectation over the clustering randomness.

*Proof sketch.* High intra-group similarity implies that experts within a group respond similarly to input tokens. When the coarse gating correctly identifies the group containing the highest-scoring expert (which occurs when inter-group separation exceeds intra-group variation), the fine-grained selection within that group recovers near-optimal experts. The (1-α) factor accounts for the approximation error when the optimal expert's score is approximated by other experts in its group. The complete proof requires bounding the probability of coarse selection error and the score gap within groups.

**Limitations.** The clustering must be recomputed periodically as expert weights evolve during training, adding overhead. The two-stage approach cannot recover from coarse selection errors—if the wrong group is selected, the optimal expert is excluded regardless of fine-grained selection quality. Performance degrades when expert specialization does not align with weight-space clustering.

### Learned Hash Routing

Learned hash routing maps tokens to experts through a learned hash function, providing fast routing at the cost of reduced adaptability.

**Algorithm Description.** We learn a hash function h: ℝ^d → {0,1}^b that maps token embeddings to b-bit binary codes. The hash function is parameterized as h(x) = sign(W_h x) where W_h ∈ ℝ^{b×d}. We maintain a lookup table T: {0,1}^b → 2^E mapping each possible hash code to a precomputed set of k experts. During inference, routing requires O(bd) operations for hash computation plus O(1) for table lookup, yielding O(bd) total complexity. Setting b = O(log n) gives O(d log n) inference complexity.

During training, we use a differentiable relaxation h_τ(x) = tanh(W_h x / τ) with temperature τ, and learn both the hash function parameters W_h and the lookup table assignments T jointly with the model parameters. The training objective includes a term encouraging hash codes to distribute uniformly across the codebook to ensure all experts receive training signal.

**Complexity Analysis.** Inference complexity is O(bd) = O(d log n) for b = O(log n) hash bits. This is sublinear in n but linear in d. The O(1) lookup table access is achieved through direct indexing with the hash code as address. Training complexity remains O(nd) as gradient computation requires evaluating the soft routing decisions across all experts.

**Load Balancing Guarantee.** Hash-based routing provides probabilistic load balancing under uniformity assumptions:

*Theorem 3 (Hash Routing Load Balance).* Assume the hash function h distributes tokens uniformly at random across the 2^b hash codes, and each hash code maps to exactly k experts with each expert appearing in exactly k · 2^b / n hash codes. Then for T tokens, the expected load on each expert is T·k/n, and the variance of expert loads is bounded by O(T·k/n) as T → ∞.

*Proof.* Under uniform hashing, each token independently selects each hash code with probability 2^{-b}. By the assignment construction, each expert is selected by a token with probability k/n. The load on expert E_i is L_i = Σ_{t=1}^T X_{ti} where X_{ti} is the indicator that token t routes to expert i. By linearity of expectation, E[L_i] = T·k/n. Since the X_{ti} are independent across tokens (though not across experts for the same token), Var(L_i) = T · (k/n) · (1 - k/n) = O(T·k/n). By Chebyshev's inequality, the load concentrates around its expectation.

**Limitations.** The uniformity assumption is strong—learned hash functions optimize for routing quality, not uniformity, creating tension between the two objectives. The lookup table size grows as 2^b, limiting b in practice. Hash collisions mean dissimilar tokens may receive identical routing, reducing model expressiveness. The discrete hash function is non-differentiable, requiring straight-through estimators or relaxations during training that introduce gradient bias.

### Summary of Complexity Bounds

We summarize the complexity results, noting the assumptions required for each:

| Method | Inference Complexity | Training Complexity | Key Assumptions |
|--------|---------------------|--------------------|-----------------| 
| Standard Top-k | O(nd) | O(nd) | None |
| Adaptive Sparse | O(d + p·nd) expected | O(nd) | Fraction p of difficult tokens |
| Hierarchical | O(√n · d) | O(nd) | Coherent expert clustering |
| Learned Hash | O(d log n) | O(nd) | Uniform hash distribution |

These bounds demonstrate that sublinear inference complexity is achievable under appropriate conditions, though each method involves tradeoffs between complexity reduction and routing quality or applicability assumptions.

## Experimental Evaluation

### Experimental Setup

We evaluated our proposed sparse routing strategies across three model scales: 1B, 7B, and 13B parameters, each configured with 32 experts per MoE layer and top-2 routing (k=2). Models were trained on a mixture of C4 (Raffel et al., 2020) and The Pile (Gao et al., 2020) datasets for 100B tokens, following established pretraining protocols. Our baseline comparisons included: (1) standard dense routing computing all gating scores (Shazeer et al., 2017), (2) BASE layers hash routing (Lewis et al., 2021), and (3) expert choice routing (Zhou et al., 2022). All experiments were conducted on NVIDIA A100 GPUs with 80GB memory, measuring performance across 1000 randomly sampled sequences of length 2048 tokens with batch size 32. We implemented our methods in PyTorch 2.0 with custom CUDA kernels for hash computation and cache lookup, ensuring fair comparison with optimized baseline implementations. Code and trained models will be released upon publication.

### Inference Efficiency Results

Table 1 presents inference latency measurements across model scales. The adaptive routing strategy achieved 2.3x speedup on the 13B model compared to standard routing, with the difficulty predictor classifying 22% of tokens as requiring full routing (p = 0.22). Hierarchical routing achieved 2.7x speedup, benefiting from the √n complexity reduction with n = 32 experts. Learned hash routing achieved 1.8x speedup; the smaller improvement reflects the O(d log n) complexity where d = 4096 dominates for moderate n.

| Model | Standard | Adaptive | Hierarchical | Hash | 
|-------|----------|----------|--------------|------|
| 1B | 312 tok/s | 624 tok/s (2.0x) | 847 tok/s (2.7x) | 531 tok/s (1.7x) |
| 7B | 156 tok/s | 327 tok/s (2.1x) | 405 tok/s (2.6x) | 265 tok/s (1.7x) |
| 13B | 89 tok/s | 205 tok/s (2.3x) | 240 tok/s (2.7x) | 160 tok/s (1.8x) |

Memory consumption decreased by 34% for the 7B model with hierarchical routing through reduced gating network activations. Peak memory usage for the 13B model was 24GB with hierarchical routing compared to 31GB for standard routing, enabling deployment on more accessible hardware configurations.

### Accuracy and Quality Metrics

Table 2 presents perplexity on held-out C4 validation data and downstream task performance. Adaptive routing achieved 12.43 perplexity versus 12.31 for standard routing on the 7B model, representing 1.0% degradation. Hierarchical routing showed 12.67 perplexity (2.9% degradation), while hash routing showed 12.89 perplexity (4.7% degradation), consistent with its coarser routing granularity.

| Method | PPL (7B) | GLUE Avg | SuperGLUE Avg |
|--------|----------|----------|---------------|
| Standard | 12.31 | 82.4 | 71.2 |
| Adaptive | 12.43 | 81.6 (-1.0%) | 70.3 (-1.3%) |
| Hierarchical | 12.67 | 80.1 (-2.8%) | 68.9 (-3.2%) |
| Hash | 12.89 | 78.7 (-4.5%) | 67.1 (-5.8%) |

Downstream task evaluation on GLUE benchmarks revealed that adaptive routing preserved 99.0% of baseline performance on average. SuperGLUE results showed greater sensitivity to routing approximation, particularly on reasoning-intensive tasks like ReCoRD where precise expert selection appears more critical. We conducted paired t-tests comparing each method against the standard baseline across 5 random seeds; differences for adaptive routing were not statistically significant at α = 0.05 (p = 0.12), while hierarchical (p = 0.03) and hash routing (p = 0.008) showed significant degradation.

### Expert Utilization and Load Balancing

We measured load balancing using the coefficient of variation (CV) of expert utilization across a batch of 10,000 tokens. Standard routing with auxiliary balancing loss achieved CV = 0.43. Adaptive routing achieved CV = 0.38, as the difficulty-based filtering did not introduce systematic expert bias. Hierarchical routing achieved CV = 0.31, benefiting from the balanced group structure. Hash routing achieved CV = 0.18, closest to the theoretical uniform distribution, though this came at the cost of routing quality as discussed above.

Expert specialization patterns emerged across all methods. Gradient-based attribution analysis revealed that early-layer experts specialized for syntactic features (part-of-speech, constituency structure) while later-layer experts captured semantic relationships. This specialization was preserved under adaptive and hierarchical routing but partially disrupted under hash routing, where the fixed hash assignments prevented dynamic expert selection based on input content.

### Ablation Studies

We conducted ablation studies on key hyperparameters using the 7B model. For adaptive routing, the difficulty threshold τ critically affected the efficiency-quality tradeoff: τ = 0.3 yielded p = 0.35 difficult tokens with 1.8x speedup and 0.5% quality loss, while τ = 0.7 yielded p = 0.12 difficult tokens with 2.8x speedup but 3.2% quality loss. The optimal operating point depends on deployment constraints.

For hierarchical routing, we varied the number of groups from 4 to 16 (with 32 total experts). Complexity scales as O(n/g + g) for g groups, minimized at g = √n ≈ 6. Empirically, g = 8 groups achieved the best efficiency-quality tradeoff, with g = 4 showing 1.2% quality improvement but 15% slower inference, and g = 16 showing 8% faster inference but 2.1% quality degradation.

For hash routing, increasing hash bits b from 4 to 8 improved routing precision (reducing quality loss from 6.2% to 4.7%) but increased lookup table size from 16 to 256 entries. Beyond b = 8, improvements were marginal while memory overhead became significant.

## Analysis and Discussion

### Efficiency-Accuracy Tradeoffs

Our experimental results reveal distinct efficiency-accuracy tradeoffs across the proposed routing strategies. Adaptive routing achieves the most favorable Pareto frontier for applications requiring minimal quality degradation, reducing computational overhead by 56% while maintaining 99% of baseline accuracy. This superior preservation of quality stems from its selective application of approximation—tokens that genuinely require precise routing receive full evaluation, while only insensitive tokens undergo approximate routing.

Hierarchical routing offers the largest speedup (2.7x) but incurs moderate quality degradation (2-3%), making it suitable for latency-critical applications where some accuracy loss is acceptable. The quality loss stems primarily from coarse selection errors where the optimal expert resides in a non-selected group; improving inter-group separation through better clustering algorithms could reduce this gap.

Hash routing provides the strongest load balancing guarantees but the weakest quality preservation, reflecting the fundamental tension between routing uniformity and input-adaptive expert selection. This method is best suited for scenarios where load balancing is paramount and moderate quality degradation is tolerable.

### Compositional Effects with Orthogonal Optimizations

We evaluated interactions between sparse routing and complementary inference optimizations. When combined with Flash Attention (Dao et al., 2022), adaptive routing achieved 3.1x end-to-end speedup compared to 2.3x from routing optimization alone, demonstrating that routing and attention optimizations provide complementary benefits. The multiplicative effect arises because reduced routing computation allows larger batch sizes within memory constraints, improving attention kernel efficiency.

INT8 quantization (Dettmers et al., 2022) maintained full compatibility with adaptive routing but introduced 1.8% additional degradation with hash routing due to quantization sensitivity in the hash function weights. We recommend applying quantization-aware training when combining hash routing with INT8 inference.

KV-cache optimization demonstrated synergistic effects with all routing methods, as sparse routing reduces the diversity of expert activations that must be cached. Adaptive routing reduced effective cache size by 23% through consistent routing of similar tokens to the same experts.

### Hardware Deployment Considerations

Optimal routing strategy selection depends on hardware architecture and deployment scenario. GPU deployments favor adaptive routing due to efficient support for dynamic branching and variable-length computation. TPU environments, which prefer static computation graphs, benefit from hash routing's fixed routing decisions that can be compiled ahead of time.

Batch size significantly influences routing overhead. At batch sizes below 16, routing computation constitutes 8-12% of total inference latency, making routing optimization impactful. Beyond batch size 64, routing overhead drops to 2-3% as expert computation dominates, reducing the relative benefit of sparse routing. For high-throughput batch processing, the simpler hash routing may be preferred despite lower quality, while interactive applications with small batches benefit more from adaptive routing.

### Limitations and Future Work

Our approaches exhibit several limitations that suggest directions for future research. First, the theoretical guarantees rely on distributional assumptions (bounded variance for adaptive routing, clustering coherence for hierarchical routing, hash uniformity for hash routing) that may not hold in all deployment scenarios. Developing routing methods with worst-case guarantees remains an open challenge.

Second, routing overhead becomes proportionally smaller for very large expert networks where expert computation dominates. For models with expert hidden dimensions exceeding 16K, the O(nd) routing cost is negligible compared to O(d·d_expert) expert computation, reducing the practical impact of sparse routing.

Third, our methods were evaluated on language modeling tasks; extension to multimodal MoE systems where routing decisions span heterogeneous expert types (vision, language, audio) requires additional investigation. The clustering and hashing approaches may need modality-specific adaptations.

Fourth, the interaction between sparse routing and expert training dynamics deserves further study. Approximate routing during training could accelerate expert specialization or conversely lead to suboptimal expert development; we used full routing during training in all experiments.

## Conclusion

This work introduces three sparse routing strategies for mixture-of-experts architectures that achieve substantial inference acceleration while maintaining model quality. Our theoretical framework establishes complexity bounds under explicitly stated assumptions: adaptive routing achieves O(d + p·nd) expected complexity for fraction p of difficult tokens, hierarchical routing achieves O(√n·d) worst-case complexity given coherent expert clustering, and hash routing achieves O(d log n) complexity with probabilistic load balancing guarantees under uniform hashing assumptions.

Experimental validation across language models from 1B to 13B parameters confirms 1.8x to 2.7x speedup with quality degradation ranging from 1% (adaptive) to 5% (hash), demonstrating practical applicability across the efficiency-quality tradeoff spectrum. Load balancing metrics show all methods maintain expert utilization coefficient of variation below 0.45, preventing the expert collapse observed in naive routing approaches.

Future research directions include developing routing methods with worst-case complexity guarantees independent of distributional assumptions, extending sparse routing to multimodal MoE architectures, and investigating the interaction between approximate routing and expert specialization during training. As MoE architectures scale to thousands of experts, efficient routing will become increasingly critical for practical deployment.

## References

Clark, A., de Las Casas, D., Guy, A., Mensch, A., Paganini, M., Hoffmann, J., Damoc, B., Heber, B., Comanescu, R., Anil, C., Aitchison, L., et al. (2022). Unified scaling laws for routed language models. In *International Conference on Machine Learning*, pp. 4057-4086. PMLR.

Cormen, T. H., Leiserson, C. E., Rivest, R. L., & Stein, C. (2009). *Introduction to Algorithms* (3rd ed.). MIT Press.

Dao, T., Fu, D., Ermon, S., Rudra, A., & Ré, C. (2022). FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In *Advances in Neural Information Processing Systems*, 35, 16344-16359.

Dettmers, T., Lewis, M., Belkada, Y., & Zettlemoyer, L. (2022). GPT3.int8(): 8-bit matrix multiplication for transformers at scale. In *Advances in Neural Information Processing Systems*, 35, 30318-30332.

Du, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., Xu, Y., Krikun, M., Zhou, Y., Yu, A. W., Firat, O., et al. (2022). GLaM: Efficient scaling of language models with mixture-of-experts. In *International Conference on Machine Learning*, pp. 5547-5569. PMLR.

Fedus, W., Zoph, B., & Shazeer, N. (2022). Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. *Journal of Machine Learning Research*, 23(120), 1-39.

Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., et al. (2020). The Pile: An 800GB dataset of diverse text for language modeling. *arXiv preprint arXiv:2101.00027*.

Hwang, C., Cui, W., Xiong, Y., Yang, Z., Liu, Z., Hu, H., Wang, Z., Salas, R., Jose, J., Ram, P., et al. (2023). Tutel: Adaptive mixture-of-experts at scale. In *Proceedings of Machine Learning and Systems*, 5, 269-281.

Kitaev, N., Kaiser, Ł., & Levskaya, A. (2020). Reformer: The efficient transformer. In *International Conference on Learning Representations*.

Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., Krikun, M., Shazeer, N., & Chen, Z. (2021). GShard: Scaling giant models with conditional computation and automatic sharding. In *International Conference on Learning Representations*.

Lewis, M., Bhosale, S., Dettmers, T., Goyal, N., & Zettlemoyer, L. (2021). BASE layers: Simplifying training of large, sparse models. In *International Conference on Machine Learning*, pp. 6265-6274. PMLR.

Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., & Liu, P. J. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer. *Journal of Machine Learning Research*, 21(140), 1-67.

Riquelme, C., Puigcerver, J., Mustafa, B., Neumann, M., Jenatton, R., Susano Pinto, A., Keysers, D., & Houlsby, N. (2021). Scaling vision with sparse mixture of experts. In *Advances in Neural Information Processing Systems*, 34, 8583-8595.

Roller, S., Sukhbaatar, S., Weston, J., et al. (2021). Hash layers for large sparse models. In *Advances in Neural Information Processing Systems*, 34, 17555-17566.

Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., & Dean, J. (2017). Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In *International Conference on Learning Representations*.

Zhou, Y., Lei, T., Liu, H., Du, N., Huang, Y., Zhao, V., Dai, A. M., Chen, Z., Le, Q. V., & Laudon, J. (2022). Mixture-of-experts with expert choice routing. In *Advances in Neural Information Processing Systems*, 35, 7103-7114.

Zoph, B., Bello, I., Kumar, S., Du, N., Huang, Y., Dean, J., Shazeer, N., & Fedus, W. (2022). ST-MoE: Designing stable and transferable sparse expert models. *arXiv preprint arXiv:2202.08906*.